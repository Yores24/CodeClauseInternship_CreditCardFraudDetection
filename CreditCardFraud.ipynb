{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Machine Learning techniques on the Credit card fraud dataset\n",
    "Dataset used: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"D:\\codeclause\\creditcard.csv\")\n",
    "# Import data based on the location you have downloaded it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1a76e4259c0>,\n",
       "  <matplotlib.axis.XTick at 0x1a76e425990>],\n",
       " [Text(0, 0, 'Normal'), Text(1, 0, 'Fraud')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Labels=[\"Normal\",\"Fraud\"]\n",
    "count=pd.value_counts(df['Class'],sort=True)\n",
    "count.plot(kind='bar')\n",
    "plt.title(\"Distribution class\")\n",
    "plt.xticks(range(2),Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scal=StandardScaler()\n",
    "X=df.drop('Class',axis=1)\n",
    "y=df['Class']\n",
    "\n",
    "X_trainv,X_test,y_trainv,y_test=train_test_split(X,y,test_size=0.3,random_state=202)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_trainv, y_trainv, \n",
    "                                                            test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud transaction weight:0.0016928854919713338\n",
      "Normal Transaction weight:0.9983071145080287\n"
     ]
    }
   ],
   "source": [
    "X_train=scal.fit_transform(X_train)\n",
    "X_validate=scal.transform(X_validate)\n",
    "X_test=scal.transform(X_test)\n",
    "\n",
    "temp1=y_train.value_counts()[0]/len(y_train)\n",
    "temp2=y_train.value_counts()[1]/len(y_train)\n",
    "print(f\"Fraud transaction weight:{temp2}\")\n",
    "print(f\"Normal Transaction weight:{temp1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "\n",
    "# Function to find the model accuracy and effectiveness\n",
    "\n",
    "def acc_score(y_fin,y_pred,train=True):\n",
    "    if train:\n",
    "        clfr=pd.DataFrame(classification_report(y_fin,y_pred, output_dict=True))\n",
    "        print(f\"Accuracy for train: {accuracy_score(y_fin,y_pred)*100:.2f}\")\n",
    "        print()\n",
    "        print(f\"Classification Report:{clfr}\")\n",
    "        print()\n",
    "        print(f\"Confusion Mat: \\n {confusion_matrix(y_train,y_pred)}\")\n",
    "    elif train==False:\n",
    "        clfr=pd.DataFrame(classification_report(y_fin,y_pred,output_dict=True))\n",
    "        print(\"Result for test data\")\n",
    "        print(f\"Accuracy for test: {accuracy_score(y_fin,y_pred)*100:.2f}\")\n",
    "        print()\n",
    "        print(f\"Classification Report:{clfr}\")\n",
    "        print()\n",
    "        print(f\"Confusion Mat: \\n {confusion_matrix(y_fin,y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are using various Neural networks and comparing how they are working on the particular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Neural Networks\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=keras.Sequential([keras.layers.Dense(256,activation='relu',input_shape=(X_train.shape[-1],)),\n",
    "                         keras.layers.BatchNormalization(),\n",
    "                         keras.layers.Dropout(0.3),\n",
    "                         keras.layers.Dense(256,activation='relu'),\n",
    "                         keras.layers.BatchNormalization(),\n",
    "                         keras.layers.Dropout(0.3),\n",
    "                         keras.layers.Dense(256,activation=\"relu\"),\n",
    "                         keras.layers.BatchNormalization(),\n",
    "                         keras.layers.Dropout(0.3),\n",
    "                         keras.layers.Dense(1,activation='sigmoid')   \n",
    "                        \n",
    "                         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the accuracy \n",
    "opti=[\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.Precision(name='p'),\n",
    "    keras.metrics.Recall(name='R')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "78/78 [==============================] - 9s 64ms/step - loss: 0.7974 - fn: 65.0000 - fp: 72879.0000 - tn: 86342.0000 - tp: 205.0000 - p: 0.0028 - R: 0.7593 - val_loss: 0.5553 - val_fn: 9.0000 - val_fp: 151.0000 - val_tn: 39664.0000 - val_tp: 49.0000 - val_p: 0.2450 - val_R: 0.8448\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 5s 61ms/step - loss: 0.6865 - fn: 39.0000 - fp: 60930.0000 - tn: 98291.0000 - tp: 231.0000 - p: 0.0038 - R: 0.8556 - val_loss: 0.4828 - val_fn: 9.0000 - val_fp: 72.0000 - val_tn: 39743.0000 - val_tp: 49.0000 - val_p: 0.4050 - val_R: 0.8448\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.6132 - fn: 42.0000 - fp: 48434.0000 - tn: 110787.0000 - tp: 228.0000 - p: 0.0047 - R: 0.8444 - val_loss: 0.4414 - val_fn: 9.0000 - val_fp: 45.0000 - val_tn: 39770.0000 - val_tp: 49.0000 - val_p: 0.5213 - val_R: 0.8448\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.5476 - fn: 34.0000 - fp: 36736.0000 - tn: 122485.0000 - tp: 236.0000 - p: 0.0064 - R: 0.8741 - val_loss: 0.3862 - val_fn: 9.0000 - val_fp: 31.0000 - val_tn: 39784.0000 - val_tp: 49.0000 - val_p: 0.6125 - val_R: 0.8448\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.4813 - fn: 41.0000 - fp: 25151.0000 - tn: 134070.0000 - tp: 229.0000 - p: 0.0090 - R: 0.8481 - val_loss: 0.3450 - val_fn: 9.0000 - val_fp: 19.0000 - val_tn: 39796.0000 - val_tp: 49.0000 - val_p: 0.7206 - val_R: 0.8448\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.4194 - fn: 45.0000 - fp: 16148.0000 - tn: 143073.0000 - tp: 225.0000 - p: 0.0137 - R: 0.8333 - val_loss: 0.3039 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.3588 - fn: 48.0000 - fp: 9709.0000 - tn: 149512.0000 - tp: 222.0000 - p: 0.0224 - R: 0.8222 - val_loss: 0.2633 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.3048 - fn: 49.0000 - fp: 5838.0000 - tn: 153383.0000 - tp: 221.0000 - p: 0.0365 - R: 0.8185 - val_loss: 0.2257 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.2565 - fn: 50.0000 - fp: 3444.0000 - tn: 155777.0000 - tp: 220.0000 - p: 0.0600 - R: 0.8148 - val_loss: 0.1826 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.2130 - fn: 57.0000 - fp: 2059.0000 - tn: 157162.0000 - tp: 213.0000 - p: 0.0938 - R: 0.7889 - val_loss: 0.1503 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.1762 - fn: 61.0000 - fp: 1394.0000 - tn: 157827.0000 - tp: 209.0000 - p: 0.1304 - R: 0.7741 - val_loss: 0.1276 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.1485 - fn: 68.0000 - fp: 964.0000 - tn: 158257.0000 - tp: 202.0000 - p: 0.1732 - R: 0.7481 - val_loss: 0.1071 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 6s 71ms/step - loss: 0.1247 - fn: 74.0000 - fp: 665.0000 - tn: 158556.0000 - tp: 196.0000 - p: 0.2276 - R: 0.7259 - val_loss: 0.0900 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.1059 - fn: 70.0000 - fp: 585.0000 - tn: 158636.0000 - tp: 200.0000 - p: 0.2548 - R: 0.7407 - val_loss: 0.0774 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0910 - fn: 77.0000 - fp: 487.0000 - tn: 158734.0000 - tp: 193.0000 - p: 0.2838 - R: 0.7148 - val_loss: 0.0658 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0785 - fn: 83.0000 - fp: 342.0000 - tn: 158879.0000 - tp: 187.0000 - p: 0.3535 - R: 0.6926 - val_loss: 0.0565 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0675 - fn: 79.0000 - fp: 325.0000 - tn: 158896.0000 - tp: 191.0000 - p: 0.3702 - R: 0.7074 - val_loss: 0.0503 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 7s 86ms/step - loss: 0.0598 - fn: 90.0000 - fp: 268.0000 - tn: 158953.0000 - tp: 180.0000 - p: 0.4018 - R: 0.6667 - val_loss: 0.0463 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0533 - fn: 79.0000 - fp: 235.0000 - tn: 158986.0000 - tp: 191.0000 - p: 0.4484 - R: 0.7074 - val_loss: 0.0427 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0476 - fn: 73.0000 - fp: 216.0000 - tn: 159005.0000 - tp: 197.0000 - p: 0.4770 - R: 0.7296 - val_loss: 0.0360 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0433 - fn: 86.0000 - fp: 168.0000 - tn: 159053.0000 - tp: 184.0000 - p: 0.5227 - R: 0.6815 - val_loss: 0.0353 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0382 - fn: 79.0000 - fp: 130.0000 - tn: 159091.0000 - tp: 191.0000 - p: 0.5950 - R: 0.7074 - val_loss: 0.0330 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0363 - fn: 81.0000 - fp: 178.0000 - tn: 159043.0000 - tp: 189.0000 - p: 0.5150 - R: 0.7000 - val_loss: 0.0307 - val_fn: 10.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 48.0000 - val_p: 0.8000 - val_R: 0.8276\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0319 - fn: 80.0000 - fp: 122.0000 - tn: 159099.0000 - tp: 190.0000 - p: 0.6090 - R: 0.7037 - val_loss: 0.0279 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0309 - fn: 84.0000 - fp: 128.0000 - tn: 159093.0000 - tp: 186.0000 - p: 0.5924 - R: 0.6889 - val_loss: 0.0280 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 4s 47ms/step - loss: 0.0293 - fn: 76.0000 - fp: 112.0000 - tn: 159109.0000 - tp: 194.0000 - p: 0.6340 - R: 0.7185 - val_loss: 0.0254 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0269 - fn: 86.0000 - fp: 120.0000 - tn: 159101.0000 - tp: 184.0000 - p: 0.6053 - R: 0.6815 - val_loss: 0.0249 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0251 - fn: 79.0000 - fp: 85.0000 - tn: 159136.0000 - tp: 191.0000 - p: 0.6920 - R: 0.7074 - val_loss: 0.0232 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0251 - fn: 83.0000 - fp: 105.0000 - tn: 159116.0000 - tp: 187.0000 - p: 0.6404 - R: 0.6926 - val_loss: 0.0254 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0232 - fn: 79.0000 - fp: 92.0000 - tn: 159129.0000 - tp: 191.0000 - p: 0.6749 - R: 0.7074 - val_loss: 0.0210 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0229 - fn: 77.0000 - fp: 106.0000 - tn: 159115.0000 - tp: 193.0000 - p: 0.6455 - R: 0.7148 - val_loss: 0.0186 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0211 - fn: 89.0000 - fp: 76.0000 - tn: 159145.0000 - tp: 181.0000 - p: 0.7043 - R: 0.6704 - val_loss: 0.0226 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0183 - fn: 70.0000 - fp: 77.0000 - tn: 159144.0000 - tp: 200.0000 - p: 0.7220 - R: 0.7407 - val_loss: 0.0177 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0185 - fn: 86.0000 - fp: 86.0000 - tn: 159135.0000 - tp: 184.0000 - p: 0.6815 - R: 0.6815 - val_loss: 0.0182 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 35/100\n",
      "78/78 [==============================] - 4s 47ms/step - loss: 0.0185 - fn: 81.0000 - fp: 63.0000 - tn: 159158.0000 - tp: 189.0000 - p: 0.7500 - R: 0.7000 - val_loss: 0.0187 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 36/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0164 - fn: 69.0000 - fp: 56.0000 - tn: 159165.0000 - tp: 201.0000 - p: 0.7821 - R: 0.7444 - val_loss: 0.0160 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 37/100\n",
      "78/78 [==============================] - 4s 48ms/step - loss: 0.0155 - fn: 82.0000 - fp: 62.0000 - tn: 159159.0000 - tp: 188.0000 - p: 0.7520 - R: 0.6963 - val_loss: 0.0159 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 38/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0158 - fn: 76.0000 - fp: 65.0000 - tn: 159156.0000 - tp: 194.0000 - p: 0.7490 - R: 0.7185 - val_loss: 0.0164 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 39/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0161 - fn: 80.0000 - fp: 71.0000 - tn: 159150.0000 - tp: 190.0000 - p: 0.7280 - R: 0.7037 - val_loss: 0.0155 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 40/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0150 - fn: 83.0000 - fp: 58.0000 - tn: 159163.0000 - tp: 187.0000 - p: 0.7633 - R: 0.6926 - val_loss: 0.0144 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 41/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0143 - fn: 78.0000 - fp: 60.0000 - tn: 159161.0000 - tp: 192.0000 - p: 0.7619 - R: 0.7111 - val_loss: 0.0128 - val_fn: 10.0000 - val_fp: 11.0000 - val_tn: 39804.0000 - val_tp: 48.0000 - val_p: 0.8136 - val_R: 0.8276\n",
      "Epoch 42/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0148 - fn: 84.0000 - fp: 61.0000 - tn: 159160.0000 - tp: 186.0000 - p: 0.7530 - R: 0.6889 - val_loss: 0.0126 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 43/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0118 - fn: 78.0000 - fp: 51.0000 - tn: 159170.0000 - tp: 192.0000 - p: 0.7901 - R: 0.7111 - val_loss: 0.0131 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 44/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0118 - fn: 72.0000 - fp: 52.0000 - tn: 159169.0000 - tp: 198.0000 - p: 0.7920 - R: 0.7333 - val_loss: 0.0126 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 45/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0114 - fn: 78.0000 - fp: 55.0000 - tn: 159166.0000 - tp: 192.0000 - p: 0.7773 - R: 0.7111 - val_loss: 0.0154 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 46/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0103 - fn: 72.0000 - fp: 51.0000 - tn: 159170.0000 - tp: 198.0000 - p: 0.7952 - R: 0.7333 - val_loss: 0.0115 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 47/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0106 - fn: 75.0000 - fp: 51.0000 - tn: 159170.0000 - tp: 195.0000 - p: 0.7927 - R: 0.7222 - val_loss: 0.0140 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 48/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.0099 - fn: 70.0000 - fp: 44.0000 - tn: 159177.0000 - tp: 200.0000 - p: 0.8197 - R: 0.7407 - val_loss: 0.0133 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 49/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0106 - fn: 74.0000 - fp: 49.0000 - tn: 159172.0000 - tp: 196.0000 - p: 0.8000 - R: 0.7259 - val_loss: 0.0113 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 50/100\n",
      "78/78 [==============================] - 5s 58ms/step - loss: 0.0090 - fn: 71.0000 - fp: 51.0000 - tn: 159170.0000 - tp: 199.0000 - p: 0.7960 - R: 0.7370 - val_loss: 0.0100 - val_fn: 9.0000 - val_fp: 11.0000 - val_tn: 39804.0000 - val_tp: 49.0000 - val_p: 0.8167 - val_R: 0.8448\n",
      "Epoch 51/100\n",
      "78/78 [==============================] - 5s 58ms/step - loss: 0.0090 - fn: 80.0000 - fp: 46.0000 - tn: 159175.0000 - tp: 190.0000 - p: 0.8051 - R: 0.7037 - val_loss: 0.0102 - val_fn: 9.0000 - val_fp: 11.0000 - val_tn: 39804.0000 - val_tp: 49.0000 - val_p: 0.8167 - val_R: 0.8448\n",
      "Epoch 52/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0100 - fn: 76.0000 - fp: 44.0000 - tn: 159177.0000 - tp: 194.0000 - p: 0.8151 - R: 0.7185 - val_loss: 0.0101 - val_fn: 9.0000 - val_fp: 11.0000 - val_tn: 39804.0000 - val_tp: 49.0000 - val_p: 0.8167 - val_R: 0.8448\n",
      "Epoch 53/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0095 - fn: 76.0000 - fp: 42.0000 - tn: 159179.0000 - tp: 194.0000 - p: 0.8220 - R: 0.7185 - val_loss: 0.0115 - val_fn: 9.0000 - val_fp: 12.0000 - val_tn: 39803.0000 - val_tp: 49.0000 - val_p: 0.8033 - val_R: 0.8448\n",
      "Epoch 54/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0086 - fn: 76.0000 - fp: 39.0000 - tn: 159182.0000 - tp: 194.0000 - p: 0.8326 - R: 0.7185 - val_loss: 0.0090 - val_fn: 12.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 46.0000 - val_p: 0.8214 - val_R: 0.7931\n",
      "Epoch 55/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.0089 - fn: 79.0000 - fp: 42.0000 - tn: 159179.0000 - tp: 191.0000 - p: 0.8197 - R: 0.7074 - val_loss: 0.0093 - val_fn: 12.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 46.0000 - val_p: 0.8214 - val_R: 0.7931\n",
      "Epoch 56/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.0086 - fn: 74.0000 - fp: 36.0000 - tn: 159185.0000 - tp: 196.0000 - p: 0.8448 - R: 0.7259 - val_loss: 0.0091 - val_fn: 12.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 46.0000 - val_p: 0.8214 - val_R: 0.7931\n",
      "Epoch 57/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0086 - fn: 81.0000 - fp: 42.0000 - tn: 159179.0000 - tp: 189.0000 - p: 0.8182 - R: 0.7000 - val_loss: 0.0081 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 58/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0077 - fn: 72.0000 - fp: 41.0000 - tn: 159180.0000 - tp: 198.0000 - p: 0.8285 - R: 0.7333 - val_loss: 0.0079 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 59/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0081 - fn: 76.0000 - fp: 37.0000 - tn: 159184.0000 - tp: 194.0000 - p: 0.8398 - R: 0.7185 - val_loss: 0.0080 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 60/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.0076 - fn: 73.0000 - fp: 38.0000 - tn: 159183.0000 - tp: 197.0000 - p: 0.8383 - R: 0.7296 - val_loss: 0.0070 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 61/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0063 - fn: 72.0000 - fp: 32.0000 - tn: 159189.0000 - tp: 198.0000 - p: 0.8609 - R: 0.7333 - val_loss: 0.0070 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 62/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0064 - fn: 79.0000 - fp: 36.0000 - tn: 159185.0000 - tp: 191.0000 - p: 0.8414 - R: 0.7074 - val_loss: 0.0076 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 63/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0065 - fn: 80.0000 - fp: 31.0000 - tn: 159190.0000 - tp: 190.0000 - p: 0.8597 - R: 0.7037 - val_loss: 0.0082 - val_fn: 12.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 46.0000 - val_p: 0.8214 - val_R: 0.7931\n",
      "Epoch 64/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0065 - fn: 72.0000 - fp: 38.0000 - tn: 159183.0000 - tp: 198.0000 - p: 0.8390 - R: 0.7333 - val_loss: 0.0066 - val_fn: 14.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 44.0000 - val_p: 0.8302 - val_R: 0.7586\n",
      "Epoch 65/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0065 - fn: 77.0000 - fp: 34.0000 - tn: 159187.0000 - tp: 193.0000 - p: 0.8502 - R: 0.7148 - val_loss: 0.0057 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 39807.0000 - val_tp: 41.0000 - val_p: 0.8367 - val_R: 0.7069\n",
      "Epoch 66/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0056 - fn: 79.0000 - fp: 27.0000 - tn: 159194.0000 - tp: 191.0000 - p: 0.8761 - R: 0.7074 - val_loss: 0.0069 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 67/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0060 - fn: 74.0000 - fp: 26.0000 - tn: 159195.0000 - tp: 196.0000 - p: 0.8829 - R: 0.7259 - val_loss: 0.0073 - val_fn: 11.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 47.0000 - val_p: 0.8246 - val_R: 0.8103\n",
      "Epoch 68/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0062 - fn: 73.0000 - fp: 32.0000 - tn: 159189.0000 - tp: 197.0000 - p: 0.8603 - R: 0.7296 - val_loss: 0.0060 - val_fn: 14.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 44.0000 - val_p: 0.8302 - val_R: 0.7586\n",
      "Epoch 69/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.0057 - fn: 70.0000 - fp: 34.0000 - tn: 159187.0000 - tp: 200.0000 - p: 0.8547 - R: 0.7407 - val_loss: 0.0065 - val_fn: 13.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 45.0000 - val_p: 0.8333 - val_R: 0.7759\n",
      "Epoch 70/100\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.0067 - fn: 75.0000 - fp: 38.0000 - tn: 159183.0000 - tp: 195.0000 - p: 0.8369 - R: 0.7222 - val_loss: 0.0051 - val_fn: 15.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 43.0000 - val_p: 0.8269 - val_R: 0.7414\n",
      "Epoch 71/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0051 - fn: 72.0000 - fp: 25.0000 - tn: 159196.0000 - tp: 198.0000 - p: 0.8879 - R: 0.7333 - val_loss: 0.0050 - val_fn: 15.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 43.0000 - val_p: 0.8269 - val_R: 0.7414\n",
      "Epoch 72/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0051 - fn: 75.0000 - fp: 26.0000 - tn: 159195.0000 - tp: 195.0000 - p: 0.8824 - R: 0.7222 - val_loss: 0.0060 - val_fn: 11.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 47.0000 - val_p: 0.8246 - val_R: 0.8103\n",
      "Epoch 73/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0054 - fn: 68.0000 - fp: 25.0000 - tn: 159196.0000 - tp: 202.0000 - p: 0.8899 - R: 0.7481 - val_loss: 0.0046 - val_fn: 17.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 41.0000 - val_p: 0.8200 - val_R: 0.7069\n",
      "Epoch 74/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0055 - fn: 79.0000 - fp: 31.0000 - tn: 159190.0000 - tp: 191.0000 - p: 0.8604 - R: 0.7074 - val_loss: 0.0053 - val_fn: 12.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 46.0000 - val_p: 0.8214 - val_R: 0.7931\n",
      "Epoch 75/100\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 0.0048 - fn: 76.0000 - fp: 31.0000 - tn: 159190.0000 - tp: 194.0000 - p: 0.8622 - R: 0.7185 - val_loss: 0.0044 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 39807.0000 - val_tp: 40.0000 - val_p: 0.8333 - val_R: 0.6897\n",
      "Epoch 76/100\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 0.0049 - fn: 72.0000 - fp: 24.0000 - tn: 159197.0000 - tp: 198.0000 - p: 0.8919 - R: 0.7333 - val_loss: 0.0046 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 42.0000 - val_p: 0.8077 - val_R: 0.7241\n",
      "Epoch 77/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0055 - fn: 76.0000 - fp: 28.0000 - tn: 159193.0000 - tp: 194.0000 - p: 0.8739 - R: 0.7185 - val_loss: 0.0043 - val_fn: 17.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 41.0000 - val_p: 0.8542 - val_R: 0.7069\n",
      "Epoch 78/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0048 - fn: 75.0000 - fp: 23.0000 - tn: 159198.0000 - tp: 195.0000 - p: 0.8945 - R: 0.7222 - val_loss: 0.0045 - val_fn: 15.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 43.0000 - val_p: 0.8113 - val_R: 0.7414\n",
      "Epoch 79/100\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 0.0044 - fn: 75.0000 - fp: 24.0000 - tn: 159197.0000 - tp: 195.0000 - p: 0.8904 - R: 0.7222 - val_loss: 0.0040 - val_fn: 19.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 39.0000 - val_p: 0.8478 - val_R: 0.6724\n",
      "Epoch 80/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0046 - fn: 77.0000 - fp: 25.0000 - tn: 159196.0000 - tp: 193.0000 - p: 0.8853 - R: 0.7148 - val_loss: 0.0040 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 40.0000 - val_p: 0.8511 - val_R: 0.6897\n",
      "Epoch 81/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0048 - fn: 80.0000 - fp: 26.0000 - tn: 159195.0000 - tp: 190.0000 - p: 0.8796 - R: 0.7037 - val_loss: 0.0040 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 39807.0000 - val_tp: 42.0000 - val_p: 0.8400 - val_R: 0.7241\n",
      "Epoch 82/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0043 - fn: 68.0000 - fp: 24.0000 - tn: 159197.0000 - tp: 202.0000 - p: 0.8938 - R: 0.7481 - val_loss: 0.0041 - val_fn: 16.0000 - val_fp: 9.0000 - val_tn: 39806.0000 - val_tp: 42.0000 - val_p: 0.8235 - val_R: 0.7241\n",
      "Epoch 83/100\n",
      "78/78 [==============================] - 4s 50ms/step - loss: 0.0041 - fn: 66.0000 - fp: 26.0000 - tn: 159195.0000 - tp: 204.0000 - p: 0.8870 - R: 0.7556 - val_loss: 0.0040 - val_fn: 15.0000 - val_fp: 10.0000 - val_tn: 39805.0000 - val_tp: 43.0000 - val_p: 0.8113 - val_R: 0.7414\n",
      "Epoch 84/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0041 - fn: 73.0000 - fp: 20.0000 - tn: 159201.0000 - tp: 197.0000 - p: 0.9078 - R: 0.7296 - val_loss: 0.0038 - val_fn: 21.0000 - val_fp: 5.0000 - val_tn: 39810.0000 - val_tp: 37.0000 - val_p: 0.8810 - val_R: 0.6379\n",
      "Epoch 85/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0039 - fn: 76.0000 - fp: 17.0000 - tn: 159204.0000 - tp: 194.0000 - p: 0.9194 - R: 0.7185 - val_loss: 0.0038 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 42.0000 - val_p: 0.8571 - val_R: 0.7241\n",
      "Epoch 86/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0046 - fn: 73.0000 - fp: 28.0000 - tn: 159193.0000 - tp: 197.0000 - p: 0.8756 - R: 0.7296 - val_loss: 0.0040 - val_fn: 21.0000 - val_fp: 5.0000 - val_tn: 39810.0000 - val_tp: 37.0000 - val_p: 0.8810 - val_R: 0.6379\n",
      "Epoch 87/100\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 0.0038 - fn: 68.0000 - fp: 17.0000 - tn: 159204.0000 - tp: 202.0000 - p: 0.9224 - R: 0.7481 - val_loss: 0.0039 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 40.0000 - val_p: 0.8511 - val_R: 0.6897\n",
      "Epoch 88/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0038 - fn: 71.0000 - fp: 19.0000 - tn: 159202.0000 - tp: 199.0000 - p: 0.9128 - R: 0.7370 - val_loss: 0.0038 - val_fn: 21.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 37.0000 - val_p: 0.8409 - val_R: 0.6379\n",
      "Epoch 89/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0035 - fn: 72.0000 - fp: 13.0000 - tn: 159208.0000 - tp: 198.0000 - p: 0.9384 - R: 0.7333 - val_loss: 0.0036 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 38.0000 - val_p: 0.8444 - val_R: 0.6552\n",
      "Epoch 90/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0036 - fn: 64.0000 - fp: 20.0000 - tn: 159201.0000 - tp: 206.0000 - p: 0.9115 - R: 0.7630 - val_loss: 0.0035 - val_fn: 21.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 37.0000 - val_p: 0.8409 - val_R: 0.6379\n",
      "Epoch 91/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0038 - fn: 73.0000 - fp: 20.0000 - tn: 159201.0000 - tp: 197.0000 - p: 0.9078 - R: 0.7296 - val_loss: 0.0035 - val_fn: 21.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 37.0000 - val_p: 0.8409 - val_R: 0.6379\n",
      "Epoch 92/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.0037 - fn: 68.0000 - fp: 18.0000 - tn: 159203.0000 - tp: 202.0000 - p: 0.9182 - R: 0.7481 - val_loss: 0.0034 - val_fn: 19.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 39.0000 - val_p: 0.8478 - val_R: 0.6724\n",
      "Epoch 93/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0035 - fn: 74.0000 - fp: 17.0000 - tn: 159204.0000 - tp: 196.0000 - p: 0.9202 - R: 0.7259 - val_loss: 0.0036 - val_fn: 19.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 39.0000 - val_p: 0.8478 - val_R: 0.6724\n",
      "Epoch 94/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0034 - fn: 66.0000 - fp: 27.0000 - tn: 159194.0000 - tp: 204.0000 - p: 0.8831 - R: 0.7556 - val_loss: 0.0033 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 38.0000 - val_p: 0.8444 - val_R: 0.6552\n",
      "Epoch 95/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0032 - fn: 68.0000 - fp: 18.0000 - tn: 159203.0000 - tp: 202.0000 - p: 0.9182 - R: 0.7481 - val_loss: 0.0034 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 38.0000 - val_p: 0.8444 - val_R: 0.6552\n",
      "Epoch 96/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0032 - fn: 72.0000 - fp: 14.0000 - tn: 159207.0000 - tp: 198.0000 - p: 0.9340 - R: 0.7333 - val_loss: 0.0033 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 40.0000 - val_p: 0.8511 - val_R: 0.6897\n",
      "Epoch 97/100\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 0.0035 - fn: 69.0000 - fp: 16.0000 - tn: 159205.0000 - tp: 201.0000 - p: 0.9263 - R: 0.7444 - val_loss: 0.0032 - val_fn: 14.0000 - val_fp: 8.0000 - val_tn: 39807.0000 - val_tp: 44.0000 - val_p: 0.8462 - val_R: 0.7586\n",
      "Epoch 98/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0032 - fn: 69.0000 - fp: 23.0000 - tn: 159198.0000 - tp: 201.0000 - p: 0.8973 - R: 0.7444 - val_loss: 0.0036 - val_fn: 13.0000 - val_fp: 8.0000 - val_tn: 39807.0000 - val_tp: 45.0000 - val_p: 0.8491 - val_R: 0.7759\n",
      "Epoch 99/100\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.0035 - fn: 73.0000 - fp: 19.0000 - tn: 159202.0000 - tp: 197.0000 - p: 0.9120 - R: 0.7296 - val_loss: 0.0033 - val_fn: 14.0000 - val_fp: 8.0000 - val_tn: 39807.0000 - val_tp: 44.0000 - val_p: 0.8462 - val_R: 0.7586\n",
      "Epoch 100/100\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 0.0035 - fn: 64.0000 - fp: 20.0000 - tn: 159201.0000 - tp: 206.0000 - p: 0.9115 - R: 0.7630 - val_loss: 0.0032 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 39808.0000 - val_tp: 43.0000 - val_p: 0.8600 - val_R: 0.7414\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer=keras.optimizers.Adam(1e-4),loss='binary_crossentropy',metrics=opti)\n",
    "cbacks= [keras.callbacks.ModelCheckpoint(\"model1_epoch_{epoch}.h5\")]\n",
    "cweight={0:temp1, 1:temp2}\n",
    "\n",
    "fin=model1.fit(X_train,y_train,validation_data=(X_validate,y_validate),batch_size=2048,epochs=100,\n",
    "               callbacks=cbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2671/2671 [==============================] - 7s 3ms/step - loss: 0.0053 - fn: 46.0000 - fp: 17.0000 - tn: 85262.0000 - tp: 118.0000 - p: 0.8741 - R: 0.7195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005279316566884518,\n",
       " 46.0,\n",
       " 17.0,\n",
       " 85262.0,\n",
       " 118.0,\n",
       " 0.8740741014480591,\n",
       " 0.7195122241973877]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score=model1.evaluate(X_test,y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4985/4985 [==============================] - 12s 2ms/step\n",
      "2671/2671 [==============================] - 6s 2ms/step\n",
      "Accuracy for train: 99.95\n",
      "\n",
      "Classification Report:                       0           1  accuracy      macro avg   weighted avg\n",
      "precision       0.999617    0.945701  0.999542       0.972659       0.999526\n",
      "recall          0.999925    0.774074  0.999542       0.886999       0.999542\n",
      "f1-score        0.999771    0.851324  0.999542       0.925547       0.999519\n",
      "support    159221.000000  270.000000  0.999542  159491.000000  159491.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[159209     12]\n",
      " [    61    209]]\n",
      "Result for test data\n",
      "Accuracy for test: 99.93\n",
      "\n",
      "Classification Report:                      0           1  accuracy     macro avg  weighted avg\n",
      "precision      0.999461    0.874074  0.999263      0.936767      0.999220\n",
      "recall         0.999801    0.719512  0.999263      0.859656      0.999263\n",
      "f1-score       0.999631    0.789298  0.999263      0.894464      0.999227\n",
      "support    85279.000000  164.000000  0.999263  85443.000000  85443.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[85262    17]\n",
      " [   46   118]]\n"
     ]
    }
   ],
   "source": [
    "y_trainpred=model1.predict(X_train)\n",
    "y_testpred=model1.predict(X_test)\n",
    "\n",
    "acc_score(y_train,y_trainpred.round(),train=True)\n",
    "acc_score(y_test,y_testpred.round(),train=False)\n",
    "dictscore={\n",
    "    'ANNs':{\n",
    "        \"Train\":f1_score(y_train,y_trainpred.round()),\n",
    "        \"Test\": f1_score(y_test,y_testpred.round())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for train: 100.00\n",
      "\n",
      "Classification Report:                       0           1  accuracy      macro avg   weighted avg\n",
      "precision       0.999994    1.000000  0.999994       0.999997       0.999994\n",
      "recall          1.000000    0.996296  0.999994       0.998148       0.999994\n",
      "f1-score        0.999997    0.998145  0.999994       0.999071       0.999994\n",
      "support    159221.000000  270.000000  0.999994  159491.000000  159491.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[159221      0]\n",
      " [     1    269]]\n",
      "Result for test data\n",
      "Accuracy for test: 99.94\n",
      "\n",
      "Classification Report:                      0          1  accuracy     macro avg  weighted avg\n",
      "precision      0.999566    0.92029  0.999438      0.959928      0.999414\n",
      "recall         0.999871    0.77439  0.999438      0.887131      0.999438\n",
      "f1-score       0.999719    0.84106  0.999438      0.920389      0.999414\n",
      "support    85279.000000  164.00000  0.999438  85443.000000  85443.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[85268    11]\n",
      " [   37   127]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(n_estimators=110,oob_score=False)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_trainpred=rfc.predict(X_train)\n",
    "y_testpred=rfc.predict(X_test)\n",
    "\n",
    "acc_score(y_train,y_trainpred,train=True)\n",
    "acc_score(y_test,y_testpred,train=False)\n",
    "dictscore['Randomforest']={\n",
    "    \"Train\":f1_score(y_train,y_trainpred),\n",
    "    \"Test\":f1_score(y_test,y_testpred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DepthWise tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yores\\AppData\\Local\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for train: 100.00\n",
      "\n",
      "Classification Report:                  0      1  accuracy  macro avg  weighted avg\n",
      "precision       1.0    1.0       1.0        1.0           1.0\n",
      "recall          1.0    1.0       1.0        1.0           1.0\n",
      "f1-score        1.0    1.0       1.0        1.0           1.0\n",
      "support    159221.0  270.0       1.0   159491.0      159491.0\n",
      "\n",
      "Confusion Mat: \n",
      " [[159221      0]\n",
      " [     0    270]]\n",
      "Result for test data\n",
      "Accuracy for test: 99.95\n",
      "\n",
      "Classification Report:                      0           1  accuracy     macro avg  weighted avg\n",
      "precision      0.999566    0.947761  0.999485      0.973664      0.999467\n",
      "recall         0.999918    0.774390  0.999485      0.887154      0.999485\n",
      "f1-score       0.999742    0.852349  0.999485      0.926046      0.999459\n",
      "support    85279.000000  164.000000  0.999485  85443.000000  85443.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[85272     7]\n",
      " [   37   127]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train,eval_metric='aucpr')\n",
    "y_trainpred=xgb.predict(X_train)\n",
    "y_testpred=xgb.predict(X_test)\n",
    "\n",
    "acc_score(y_train,y_trainpred,train=True)\n",
    "acc_score(y_test,y_testpred,train=False)\n",
    "dictscore['XGB']={\n",
    "    \"Train\":f1_score(y_train,y_trainpred),\n",
    "    \"Test\":f1_score(y_test,y_testpred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.089847\n",
      "0:\tlearn: 0.3887254\ttotal: 201ms\tremaining: 3m 21s\n",
      "1:\tlearn: 0.2343837\ttotal: 245ms\tremaining: 2m 2s\n",
      "2:\tlearn: 0.1343697\ttotal: 287ms\tremaining: 1m 35s\n",
      "3:\tlearn: 0.0784671\ttotal: 325ms\tremaining: 1m 20s\n",
      "4:\tlearn: 0.0477819\ttotal: 365ms\tremaining: 1m 12s\n",
      "5:\tlearn: 0.0306784\ttotal: 403ms\tremaining: 1m 6s\n",
      "6:\tlearn: 0.0206532\ttotal: 443ms\tremaining: 1m 2s\n",
      "7:\tlearn: 0.0147317\ttotal: 480ms\tremaining: 59.5s\n",
      "8:\tlearn: 0.0109480\ttotal: 517ms\tremaining: 56.9s\n",
      "9:\tlearn: 0.0085323\ttotal: 552ms\tremaining: 54.7s\n",
      "10:\tlearn: 0.0069154\ttotal: 589ms\tremaining: 52.9s\n",
      "11:\tlearn: 0.0058227\ttotal: 624ms\tremaining: 51.3s\n",
      "12:\tlearn: 0.0050066\ttotal: 662ms\tremaining: 50.2s\n",
      "13:\tlearn: 0.0044171\ttotal: 697ms\tremaining: 49.1s\n",
      "14:\tlearn: 0.0039794\ttotal: 734ms\tremaining: 48.2s\n",
      "15:\tlearn: 0.0036600\ttotal: 769ms\tremaining: 47.3s\n",
      "16:\tlearn: 0.0034172\ttotal: 806ms\tremaining: 46.6s\n",
      "17:\tlearn: 0.0032331\ttotal: 843ms\tremaining: 46s\n",
      "18:\tlearn: 0.0030724\ttotal: 881ms\tremaining: 45.5s\n",
      "19:\tlearn: 0.0029108\ttotal: 918ms\tremaining: 45s\n",
      "20:\tlearn: 0.0028234\ttotal: 957ms\tremaining: 44.6s\n",
      "21:\tlearn: 0.0027379\ttotal: 994ms\tremaining: 44.2s\n",
      "22:\tlearn: 0.0026458\ttotal: 1.03s\tremaining: 43.8s\n",
      "23:\tlearn: 0.0025962\ttotal: 1.07s\tremaining: 43.4s\n",
      "24:\tlearn: 0.0025353\ttotal: 1.1s\tremaining: 43.1s\n",
      "25:\tlearn: 0.0024874\ttotal: 1.14s\tremaining: 42.8s\n",
      "26:\tlearn: 0.0024625\ttotal: 1.18s\tremaining: 42.4s\n",
      "27:\tlearn: 0.0024194\ttotal: 1.22s\tremaining: 42.2s\n",
      "28:\tlearn: 0.0024025\ttotal: 1.25s\tremaining: 41.9s\n",
      "29:\tlearn: 0.0023779\ttotal: 1.29s\tremaining: 41.6s\n",
      "30:\tlearn: 0.0023596\ttotal: 1.32s\tremaining: 41.4s\n",
      "31:\tlearn: 0.0023147\ttotal: 1.36s\tremaining: 41.3s\n",
      "32:\tlearn: 0.0022829\ttotal: 1.39s\tremaining: 40.8s\n",
      "33:\tlearn: 0.0022655\ttotal: 1.42s\tremaining: 40.3s\n",
      "34:\tlearn: 0.0022447\ttotal: 1.45s\tremaining: 39.9s\n",
      "35:\tlearn: 0.0022309\ttotal: 1.47s\tremaining: 39.4s\n",
      "36:\tlearn: 0.0022186\ttotal: 1.5s\tremaining: 39.1s\n",
      "37:\tlearn: 0.0022085\ttotal: 1.53s\tremaining: 38.7s\n",
      "38:\tlearn: 0.0021735\ttotal: 1.55s\tremaining: 38.3s\n",
      "39:\tlearn: 0.0021545\ttotal: 1.58s\tremaining: 38s\n",
      "40:\tlearn: 0.0021378\ttotal: 1.61s\tremaining: 37.7s\n",
      "41:\tlearn: 0.0021234\ttotal: 1.64s\tremaining: 37.3s\n",
      "42:\tlearn: 0.0020922\ttotal: 1.66s\tremaining: 37s\n",
      "43:\tlearn: 0.0020665\ttotal: 1.69s\tremaining: 36.7s\n",
      "44:\tlearn: 0.0020493\ttotal: 1.72s\tremaining: 36.4s\n",
      "45:\tlearn: 0.0020280\ttotal: 1.75s\tremaining: 36.2s\n",
      "46:\tlearn: 0.0020026\ttotal: 1.77s\tremaining: 36s\n",
      "47:\tlearn: 0.0019752\ttotal: 1.8s\tremaining: 35.8s\n",
      "48:\tlearn: 0.0019672\ttotal: 1.83s\tremaining: 35.5s\n",
      "49:\tlearn: 0.0019503\ttotal: 1.86s\tremaining: 35.3s\n",
      "50:\tlearn: 0.0019345\ttotal: 1.89s\tremaining: 35.1s\n",
      "51:\tlearn: 0.0019224\ttotal: 1.91s\tremaining: 34.8s\n",
      "52:\tlearn: 0.0019123\ttotal: 1.94s\tremaining: 34.7s\n",
      "53:\tlearn: 0.0018994\ttotal: 1.97s\tremaining: 34.4s\n",
      "54:\tlearn: 0.0018713\ttotal: 1.99s\tremaining: 34.2s\n",
      "55:\tlearn: 0.0018522\ttotal: 2.02s\tremaining: 34.1s\n",
      "56:\tlearn: 0.0018415\ttotal: 2.05s\tremaining: 33.9s\n",
      "57:\tlearn: 0.0018336\ttotal: 2.08s\tremaining: 33.7s\n",
      "58:\tlearn: 0.0018160\ttotal: 2.1s\tremaining: 33.6s\n",
      "59:\tlearn: 0.0018024\ttotal: 2.13s\tremaining: 33.4s\n",
      "60:\tlearn: 0.0017966\ttotal: 2.16s\tremaining: 33.3s\n",
      "61:\tlearn: 0.0017892\ttotal: 2.19s\tremaining: 33.1s\n",
      "62:\tlearn: 0.0017859\ttotal: 2.22s\tremaining: 33s\n",
      "63:\tlearn: 0.0017777\ttotal: 2.25s\tremaining: 32.9s\n",
      "64:\tlearn: 0.0017738\ttotal: 2.27s\tremaining: 32.7s\n",
      "65:\tlearn: 0.0017690\ttotal: 2.31s\tremaining: 32.6s\n",
      "66:\tlearn: 0.0017564\ttotal: 2.33s\tremaining: 32.5s\n",
      "67:\tlearn: 0.0017483\ttotal: 2.37s\tremaining: 32.5s\n",
      "68:\tlearn: 0.0017335\ttotal: 2.4s\tremaining: 32.4s\n",
      "69:\tlearn: 0.0017221\ttotal: 2.43s\tremaining: 32.3s\n",
      "70:\tlearn: 0.0017150\ttotal: 2.46s\tremaining: 32.3s\n",
      "71:\tlearn: 0.0017044\ttotal: 2.5s\tremaining: 32.2s\n",
      "72:\tlearn: 0.0016971\ttotal: 2.53s\tremaining: 32.1s\n",
      "73:\tlearn: 0.0016902\ttotal: 2.55s\tremaining: 32s\n",
      "74:\tlearn: 0.0016790\ttotal: 2.58s\tremaining: 31.8s\n",
      "75:\tlearn: 0.0016665\ttotal: 2.61s\tremaining: 31.8s\n",
      "76:\tlearn: 0.0016583\ttotal: 2.64s\tremaining: 31.7s\n",
      "77:\tlearn: 0.0016506\ttotal: 2.67s\tremaining: 31.6s\n",
      "78:\tlearn: 0.0016376\ttotal: 2.7s\tremaining: 31.5s\n",
      "79:\tlearn: 0.0016324\ttotal: 2.73s\tremaining: 31.4s\n",
      "80:\tlearn: 0.0016255\ttotal: 2.76s\tremaining: 31.4s\n",
      "81:\tlearn: 0.0016141\ttotal: 2.79s\tremaining: 31.3s\n",
      "82:\tlearn: 0.0016071\ttotal: 2.82s\tremaining: 31.2s\n",
      "83:\tlearn: 0.0015898\ttotal: 2.85s\tremaining: 31s\n",
      "84:\tlearn: 0.0015752\ttotal: 2.89s\tremaining: 31.1s\n",
      "85:\tlearn: 0.0015684\ttotal: 2.92s\tremaining: 31s\n",
      "86:\tlearn: 0.0015612\ttotal: 2.94s\tremaining: 30.9s\n",
      "87:\tlearn: 0.0015545\ttotal: 2.97s\tremaining: 30.8s\n",
      "88:\tlearn: 0.0015448\ttotal: 3s\tremaining: 30.7s\n",
      "89:\tlearn: 0.0015326\ttotal: 3.02s\tremaining: 30.6s\n",
      "90:\tlearn: 0.0015194\ttotal: 3.05s\tremaining: 30.5s\n",
      "91:\tlearn: 0.0015143\ttotal: 3.08s\tremaining: 30.4s\n",
      "92:\tlearn: 0.0015045\ttotal: 3.1s\tremaining: 30.3s\n",
      "93:\tlearn: 0.0014968\ttotal: 3.13s\tremaining: 30.2s\n",
      "94:\tlearn: 0.0014884\ttotal: 3.16s\tremaining: 30.1s\n",
      "95:\tlearn: 0.0014788\ttotal: 3.19s\tremaining: 30s\n",
      "96:\tlearn: 0.0014758\ttotal: 3.21s\tremaining: 29.9s\n",
      "97:\tlearn: 0.0014702\ttotal: 3.24s\tremaining: 29.8s\n",
      "98:\tlearn: 0.0014555\ttotal: 3.27s\tremaining: 29.7s\n",
      "99:\tlearn: 0.0014470\ttotal: 3.29s\tremaining: 29.6s\n",
      "100:\tlearn: 0.0014411\ttotal: 3.32s\tremaining: 29.6s\n",
      "101:\tlearn: 0.0014338\ttotal: 3.36s\tremaining: 29.6s\n",
      "102:\tlearn: 0.0014285\ttotal: 3.38s\tremaining: 29.5s\n",
      "103:\tlearn: 0.0014138\ttotal: 3.42s\tremaining: 29.4s\n",
      "104:\tlearn: 0.0014095\ttotal: 3.44s\tremaining: 29.3s\n",
      "105:\tlearn: 0.0013939\ttotal: 3.46s\tremaining: 29.2s\n",
      "106:\tlearn: 0.0013742\ttotal: 3.49s\tremaining: 29.1s\n",
      "107:\tlearn: 0.0013657\ttotal: 3.52s\tremaining: 29s\n",
      "108:\tlearn: 0.0013566\ttotal: 3.55s\tremaining: 29s\n",
      "109:\tlearn: 0.0013449\ttotal: 3.58s\tremaining: 29s\n",
      "110:\tlearn: 0.0013288\ttotal: 3.6s\tremaining: 28.9s\n",
      "111:\tlearn: 0.0013197\ttotal: 3.63s\tremaining: 28.8s\n",
      "112:\tlearn: 0.0013084\ttotal: 3.66s\tremaining: 28.7s\n",
      "113:\tlearn: 0.0012979\ttotal: 3.69s\tremaining: 28.6s\n",
      "114:\tlearn: 0.0012932\ttotal: 3.71s\tremaining: 28.6s\n",
      "115:\tlearn: 0.0012852\ttotal: 3.74s\tremaining: 28.5s\n",
      "116:\tlearn: 0.0012821\ttotal: 3.77s\tremaining: 28.5s\n",
      "117:\tlearn: 0.0012728\ttotal: 3.8s\tremaining: 28.4s\n",
      "118:\tlearn: 0.0012635\ttotal: 3.82s\tremaining: 28.3s\n",
      "119:\tlearn: 0.0012598\ttotal: 3.85s\tremaining: 28.2s\n",
      "120:\tlearn: 0.0012559\ttotal: 3.87s\tremaining: 28.1s\n",
      "121:\tlearn: 0.0012520\ttotal: 3.9s\tremaining: 28s\n",
      "122:\tlearn: 0.0012486\ttotal: 3.92s\tremaining: 28s\n",
      "123:\tlearn: 0.0012412\ttotal: 3.95s\tremaining: 27.9s\n",
      "124:\tlearn: 0.0012342\ttotal: 3.98s\tremaining: 27.8s\n",
      "125:\tlearn: 0.0012280\ttotal: 4.01s\tremaining: 27.8s\n",
      "126:\tlearn: 0.0012160\ttotal: 4.03s\tremaining: 27.7s\n",
      "127:\tlearn: 0.0012009\ttotal: 4.06s\tremaining: 27.7s\n",
      "128:\tlearn: 0.0011931\ttotal: 4.08s\tremaining: 27.6s\n",
      "129:\tlearn: 0.0011877\ttotal: 4.11s\tremaining: 27.5s\n",
      "130:\tlearn: 0.0011857\ttotal: 4.14s\tremaining: 27.5s\n",
      "131:\tlearn: 0.0011777\ttotal: 4.16s\tremaining: 27.4s\n",
      "132:\tlearn: 0.0011742\ttotal: 4.19s\tremaining: 27.3s\n",
      "133:\tlearn: 0.0011627\ttotal: 4.22s\tremaining: 27.2s\n",
      "134:\tlearn: 0.0011558\ttotal: 4.24s\tremaining: 27.2s\n",
      "135:\tlearn: 0.0011488\ttotal: 4.27s\tremaining: 27.1s\n",
      "136:\tlearn: 0.0011392\ttotal: 4.29s\tremaining: 27.1s\n",
      "137:\tlearn: 0.0011238\ttotal: 4.32s\tremaining: 27s\n",
      "138:\tlearn: 0.0011184\ttotal: 4.35s\tremaining: 26.9s\n",
      "139:\tlearn: 0.0011114\ttotal: 4.38s\tremaining: 26.9s\n",
      "140:\tlearn: 0.0011074\ttotal: 4.4s\tremaining: 26.8s\n",
      "141:\tlearn: 0.0010960\ttotal: 4.43s\tremaining: 26.8s\n",
      "142:\tlearn: 0.0010933\ttotal: 4.46s\tremaining: 26.7s\n",
      "143:\tlearn: 0.0010812\ttotal: 4.49s\tremaining: 26.7s\n",
      "144:\tlearn: 0.0010791\ttotal: 4.51s\tremaining: 26.6s\n",
      "145:\tlearn: 0.0010719\ttotal: 4.54s\tremaining: 26.5s\n",
      "146:\tlearn: 0.0010644\ttotal: 4.57s\tremaining: 26.5s\n",
      "147:\tlearn: 0.0010547\ttotal: 4.59s\tremaining: 26.4s\n",
      "148:\tlearn: 0.0010468\ttotal: 4.62s\tremaining: 26.4s\n",
      "149:\tlearn: 0.0010429\ttotal: 4.65s\tremaining: 26.3s\n",
      "150:\tlearn: 0.0010392\ttotal: 4.67s\tremaining: 26.3s\n",
      "151:\tlearn: 0.0010290\ttotal: 4.7s\tremaining: 26.2s\n",
      "152:\tlearn: 0.0010244\ttotal: 4.72s\tremaining: 26.1s\n",
      "153:\tlearn: 0.0010191\ttotal: 4.75s\tremaining: 26.1s\n",
      "154:\tlearn: 0.0010112\ttotal: 4.78s\tremaining: 26s\n",
      "155:\tlearn: 0.0010053\ttotal: 4.8s\tremaining: 26s\n",
      "156:\tlearn: 0.0010016\ttotal: 4.83s\tremaining: 25.9s\n",
      "157:\tlearn: 0.0009978\ttotal: 4.86s\tremaining: 25.9s\n",
      "158:\tlearn: 0.0009945\ttotal: 4.88s\tremaining: 25.8s\n",
      "159:\tlearn: 0.0009902\ttotal: 4.91s\tremaining: 25.8s\n",
      "160:\tlearn: 0.0009841\ttotal: 4.93s\tremaining: 25.7s\n",
      "161:\tlearn: 0.0009792\ttotal: 4.96s\tremaining: 25.7s\n",
      "162:\tlearn: 0.0009725\ttotal: 4.99s\tremaining: 25.6s\n",
      "163:\tlearn: 0.0009674\ttotal: 5.01s\tremaining: 25.6s\n",
      "164:\tlearn: 0.0009620\ttotal: 5.04s\tremaining: 25.5s\n",
      "165:\tlearn: 0.0009597\ttotal: 5.06s\tremaining: 25.4s\n",
      "166:\tlearn: 0.0009565\ttotal: 5.09s\tremaining: 25.4s\n",
      "167:\tlearn: 0.0009513\ttotal: 5.12s\tremaining: 25.4s\n",
      "168:\tlearn: 0.0009473\ttotal: 5.15s\tremaining: 25.3s\n",
      "169:\tlearn: 0.0009461\ttotal: 5.17s\tremaining: 25.2s\n",
      "170:\tlearn: 0.0009439\ttotal: 5.2s\tremaining: 25.2s\n",
      "171:\tlearn: 0.0009394\ttotal: 5.22s\tremaining: 25.1s\n",
      "172:\tlearn: 0.0009332\ttotal: 5.25s\tremaining: 25.1s\n",
      "173:\tlearn: 0.0009306\ttotal: 5.27s\tremaining: 25s\n",
      "174:\tlearn: 0.0009255\ttotal: 5.3s\tremaining: 25s\n",
      "175:\tlearn: 0.0009162\ttotal: 5.33s\tremaining: 24.9s\n",
      "176:\tlearn: 0.0009117\ttotal: 5.35s\tremaining: 24.9s\n",
      "177:\tlearn: 0.0009075\ttotal: 5.38s\tremaining: 24.9s\n",
      "178:\tlearn: 0.0009031\ttotal: 5.41s\tremaining: 24.8s\n",
      "179:\tlearn: 0.0008986\ttotal: 5.43s\tremaining: 24.8s\n",
      "180:\tlearn: 0.0008954\ttotal: 5.46s\tremaining: 24.7s\n",
      "181:\tlearn: 0.0008936\ttotal: 5.49s\tremaining: 24.7s\n",
      "182:\tlearn: 0.0008915\ttotal: 5.51s\tremaining: 24.6s\n",
      "183:\tlearn: 0.0008898\ttotal: 5.54s\tremaining: 24.6s\n",
      "184:\tlearn: 0.0008853\ttotal: 5.56s\tremaining: 24.5s\n",
      "185:\tlearn: 0.0008829\ttotal: 5.59s\tremaining: 24.5s\n",
      "186:\tlearn: 0.0008787\ttotal: 5.62s\tremaining: 24.4s\n",
      "187:\tlearn: 0.0008761\ttotal: 5.64s\tremaining: 24.4s\n",
      "188:\tlearn: 0.0008722\ttotal: 5.67s\tremaining: 24.3s\n",
      "189:\tlearn: 0.0008702\ttotal: 5.69s\tremaining: 24.3s\n",
      "190:\tlearn: 0.0008693\ttotal: 5.72s\tremaining: 24.2s\n",
      "191:\tlearn: 0.0008647\ttotal: 5.74s\tremaining: 24.2s\n",
      "192:\tlearn: 0.0008611\ttotal: 5.77s\tremaining: 24.1s\n",
      "193:\tlearn: 0.0008598\ttotal: 5.79s\tremaining: 24.1s\n",
      "194:\tlearn: 0.0008573\ttotal: 5.82s\tremaining: 24s\n",
      "195:\tlearn: 0.0008537\ttotal: 5.85s\tremaining: 24s\n",
      "196:\tlearn: 0.0008504\ttotal: 5.87s\tremaining: 23.9s\n",
      "197:\tlearn: 0.0008462\ttotal: 5.9s\tremaining: 23.9s\n",
      "198:\tlearn: 0.0008451\ttotal: 5.93s\tremaining: 23.9s\n",
      "199:\tlearn: 0.0008441\ttotal: 5.95s\tremaining: 23.8s\n",
      "200:\tlearn: 0.0008387\ttotal: 5.98s\tremaining: 23.8s\n",
      "201:\tlearn: 0.0008324\ttotal: 6s\tremaining: 23.7s\n",
      "202:\tlearn: 0.0008280\ttotal: 6.03s\tremaining: 23.7s\n",
      "203:\tlearn: 0.0008217\ttotal: 6.06s\tremaining: 23.6s\n",
      "204:\tlearn: 0.0008199\ttotal: 6.08s\tremaining: 23.6s\n",
      "205:\tlearn: 0.0008166\ttotal: 6.11s\tremaining: 23.5s\n",
      "206:\tlearn: 0.0008156\ttotal: 6.13s\tremaining: 23.5s\n",
      "207:\tlearn: 0.0008149\ttotal: 6.16s\tremaining: 23.4s\n",
      "208:\tlearn: 0.0008109\ttotal: 6.19s\tremaining: 23.4s\n",
      "209:\tlearn: 0.0008101\ttotal: 6.21s\tremaining: 23.4s\n",
      "210:\tlearn: 0.0008094\ttotal: 6.24s\tremaining: 23.3s\n",
      "211:\tlearn: 0.0008051\ttotal: 6.26s\tremaining: 23.3s\n",
      "212:\tlearn: 0.0007999\ttotal: 6.29s\tremaining: 23.2s\n",
      "213:\tlearn: 0.0007967\ttotal: 6.32s\tremaining: 23.2s\n",
      "214:\tlearn: 0.0007890\ttotal: 6.34s\tremaining: 23.2s\n",
      "215:\tlearn: 0.0007884\ttotal: 6.37s\tremaining: 23.1s\n",
      "216:\tlearn: 0.0007850\ttotal: 6.4s\tremaining: 23.1s\n",
      "217:\tlearn: 0.0007815\ttotal: 6.42s\tremaining: 23.1s\n",
      "218:\tlearn: 0.0007784\ttotal: 6.45s\tremaining: 23s\n",
      "219:\tlearn: 0.0007776\ttotal: 6.48s\tremaining: 23s\n",
      "220:\tlearn: 0.0007668\ttotal: 6.5s\tremaining: 22.9s\n",
      "221:\tlearn: 0.0007661\ttotal: 6.53s\tremaining: 22.9s\n",
      "222:\tlearn: 0.0007629\ttotal: 6.56s\tremaining: 22.8s\n",
      "223:\tlearn: 0.0007621\ttotal: 6.58s\tremaining: 22.8s\n",
      "224:\tlearn: 0.0007610\ttotal: 6.6s\tremaining: 22.7s\n",
      "225:\tlearn: 0.0007563\ttotal: 6.63s\tremaining: 22.7s\n",
      "226:\tlearn: 0.0007524\ttotal: 6.66s\tremaining: 22.7s\n",
      "227:\tlearn: 0.0007490\ttotal: 6.68s\tremaining: 22.6s\n",
      "228:\tlearn: 0.0007400\ttotal: 6.71s\tremaining: 22.6s\n",
      "229:\tlearn: 0.0007370\ttotal: 6.74s\tremaining: 22.6s\n",
      "230:\tlearn: 0.0007312\ttotal: 6.76s\tremaining: 22.5s\n",
      "231:\tlearn: 0.0007275\ttotal: 6.79s\tremaining: 22.5s\n",
      "232:\tlearn: 0.0007227\ttotal: 6.81s\tremaining: 22.4s\n",
      "233:\tlearn: 0.0007198\ttotal: 6.84s\tremaining: 22.4s\n",
      "234:\tlearn: 0.0007187\ttotal: 6.86s\tremaining: 22.3s\n",
      "235:\tlearn: 0.0007108\ttotal: 6.89s\tremaining: 22.3s\n",
      "236:\tlearn: 0.0007066\ttotal: 6.92s\tremaining: 22.3s\n",
      "237:\tlearn: 0.0007028\ttotal: 6.94s\tremaining: 22.2s\n",
      "238:\tlearn: 0.0006990\ttotal: 6.97s\tremaining: 22.2s\n",
      "239:\tlearn: 0.0006979\ttotal: 7s\tremaining: 22.2s\n",
      "240:\tlearn: 0.0006906\ttotal: 7.02s\tremaining: 22.1s\n",
      "241:\tlearn: 0.0006877\ttotal: 7.05s\tremaining: 22.1s\n",
      "242:\tlearn: 0.0006819\ttotal: 7.08s\tremaining: 22s\n",
      "243:\tlearn: 0.0006779\ttotal: 7.1s\tremaining: 22s\n",
      "244:\tlearn: 0.0006754\ttotal: 7.13s\tremaining: 22s\n",
      "245:\tlearn: 0.0006730\ttotal: 7.15s\tremaining: 21.9s\n",
      "246:\tlearn: 0.0006679\ttotal: 7.17s\tremaining: 21.9s\n",
      "247:\tlearn: 0.0006653\ttotal: 7.2s\tremaining: 21.8s\n",
      "248:\tlearn: 0.0006627\ttotal: 7.22s\tremaining: 21.8s\n",
      "249:\tlearn: 0.0006578\ttotal: 7.25s\tremaining: 21.7s\n",
      "250:\tlearn: 0.0006554\ttotal: 7.27s\tremaining: 21.7s\n",
      "251:\tlearn: 0.0006530\ttotal: 7.3s\tremaining: 21.7s\n",
      "252:\tlearn: 0.0006507\ttotal: 7.32s\tremaining: 21.6s\n",
      "253:\tlearn: 0.0006503\ttotal: 7.35s\tremaining: 21.6s\n",
      "254:\tlearn: 0.0006482\ttotal: 7.37s\tremaining: 21.5s\n",
      "255:\tlearn: 0.0006471\ttotal: 7.4s\tremaining: 21.5s\n",
      "256:\tlearn: 0.0006453\ttotal: 7.43s\tremaining: 21.5s\n",
      "257:\tlearn: 0.0006433\ttotal: 7.45s\tremaining: 21.4s\n",
      "258:\tlearn: 0.0006414\ttotal: 7.48s\tremaining: 21.4s\n",
      "259:\tlearn: 0.0006401\ttotal: 7.5s\tremaining: 21.4s\n",
      "260:\tlearn: 0.0006383\ttotal: 7.53s\tremaining: 21.3s\n",
      "261:\tlearn: 0.0006372\ttotal: 7.55s\tremaining: 21.3s\n",
      "262:\tlearn: 0.0006355\ttotal: 7.58s\tremaining: 21.2s\n",
      "263:\tlearn: 0.0006340\ttotal: 7.61s\tremaining: 21.2s\n",
      "264:\tlearn: 0.0006330\ttotal: 7.63s\tremaining: 21.2s\n",
      "265:\tlearn: 0.0006315\ttotal: 7.66s\tremaining: 21.1s\n",
      "266:\tlearn: 0.0006301\ttotal: 7.68s\tremaining: 21.1s\n",
      "267:\tlearn: 0.0006291\ttotal: 7.71s\tremaining: 21.1s\n",
      "268:\tlearn: 0.0006255\ttotal: 7.74s\tremaining: 21s\n",
      "269:\tlearn: 0.0006242\ttotal: 7.77s\tremaining: 21s\n",
      "270:\tlearn: 0.0006189\ttotal: 7.8s\tremaining: 21s\n",
      "271:\tlearn: 0.0006170\ttotal: 7.82s\tremaining: 20.9s\n",
      "272:\tlearn: 0.0006158\ttotal: 7.84s\tremaining: 20.9s\n",
      "273:\tlearn: 0.0006152\ttotal: 7.87s\tremaining: 20.8s\n",
      "274:\tlearn: 0.0006143\ttotal: 7.89s\tremaining: 20.8s\n",
      "275:\tlearn: 0.0006124\ttotal: 7.92s\tremaining: 20.8s\n",
      "276:\tlearn: 0.0006084\ttotal: 7.94s\tremaining: 20.7s\n",
      "277:\tlearn: 0.0006079\ttotal: 7.96s\tremaining: 20.7s\n",
      "278:\tlearn: 0.0006042\ttotal: 7.99s\tremaining: 20.6s\n",
      "279:\tlearn: 0.0006019\ttotal: 8.01s\tremaining: 20.6s\n",
      "280:\tlearn: 0.0005995\ttotal: 8.03s\tremaining: 20.6s\n",
      "281:\tlearn: 0.0005966\ttotal: 8.05s\tremaining: 20.5s\n",
      "282:\tlearn: 0.0005922\ttotal: 8.07s\tremaining: 20.5s\n",
      "283:\tlearn: 0.0005884\ttotal: 8.1s\tremaining: 20.4s\n",
      "284:\tlearn: 0.0005878\ttotal: 8.12s\tremaining: 20.4s\n",
      "285:\tlearn: 0.0005848\ttotal: 8.14s\tremaining: 20.3s\n",
      "286:\tlearn: 0.0005813\ttotal: 8.16s\tremaining: 20.3s\n",
      "287:\tlearn: 0.0005808\ttotal: 8.18s\tremaining: 20.2s\n",
      "288:\tlearn: 0.0005780\ttotal: 8.21s\tremaining: 20.2s\n",
      "289:\tlearn: 0.0005762\ttotal: 8.23s\tremaining: 20.1s\n",
      "290:\tlearn: 0.0005752\ttotal: 8.25s\tremaining: 20.1s\n",
      "291:\tlearn: 0.0005727\ttotal: 8.27s\tremaining: 20.1s\n",
      "292:\tlearn: 0.0005707\ttotal: 8.29s\tremaining: 20s\n",
      "293:\tlearn: 0.0005664\ttotal: 8.32s\tremaining: 20s\n",
      "294:\tlearn: 0.0005625\ttotal: 8.34s\tremaining: 19.9s\n",
      "295:\tlearn: 0.0005582\ttotal: 8.36s\tremaining: 19.9s\n",
      "296:\tlearn: 0.0005554\ttotal: 8.38s\tremaining: 19.8s\n",
      "297:\tlearn: 0.0005546\ttotal: 8.4s\tremaining: 19.8s\n",
      "298:\tlearn: 0.0005521\ttotal: 8.43s\tremaining: 19.8s\n",
      "299:\tlearn: 0.0005490\ttotal: 8.45s\tremaining: 19.7s\n",
      "300:\tlearn: 0.0005485\ttotal: 8.47s\tremaining: 19.7s\n",
      "301:\tlearn: 0.0005427\ttotal: 8.49s\tremaining: 19.6s\n",
      "302:\tlearn: 0.0005404\ttotal: 8.51s\tremaining: 19.6s\n",
      "303:\tlearn: 0.0005387\ttotal: 8.53s\tremaining: 19.5s\n",
      "304:\tlearn: 0.0005356\ttotal: 8.55s\tremaining: 19.5s\n",
      "305:\tlearn: 0.0005302\ttotal: 8.58s\tremaining: 19.5s\n",
      "306:\tlearn: 0.0005250\ttotal: 8.6s\tremaining: 19.4s\n",
      "307:\tlearn: 0.0005245\ttotal: 8.62s\tremaining: 19.4s\n",
      "308:\tlearn: 0.0005239\ttotal: 8.64s\tremaining: 19.3s\n",
      "309:\tlearn: 0.0005190\ttotal: 8.67s\tremaining: 19.3s\n",
      "310:\tlearn: 0.0005182\ttotal: 8.69s\tremaining: 19.2s\n",
      "311:\tlearn: 0.0005160\ttotal: 8.71s\tremaining: 19.2s\n",
      "312:\tlearn: 0.0005155\ttotal: 8.73s\tremaining: 19.2s\n",
      "313:\tlearn: 0.0005125\ttotal: 8.75s\tremaining: 19.1s\n",
      "314:\tlearn: 0.0005118\ttotal: 8.77s\tremaining: 19.1s\n",
      "315:\tlearn: 0.0005080\ttotal: 8.79s\tremaining: 19s\n",
      "316:\tlearn: 0.0005075\ttotal: 8.82s\tremaining: 19s\n",
      "317:\tlearn: 0.0005072\ttotal: 8.84s\tremaining: 19s\n",
      "318:\tlearn: 0.0005067\ttotal: 8.86s\tremaining: 18.9s\n",
      "319:\tlearn: 0.0005063\ttotal: 8.88s\tremaining: 18.9s\n",
      "320:\tlearn: 0.0005029\ttotal: 8.9s\tremaining: 18.8s\n",
      "321:\tlearn: 0.0005026\ttotal: 8.92s\tremaining: 18.8s\n",
      "322:\tlearn: 0.0005024\ttotal: 8.94s\tremaining: 18.7s\n",
      "323:\tlearn: 0.0004955\ttotal: 8.96s\tremaining: 18.7s\n",
      "324:\tlearn: 0.0004951\ttotal: 8.98s\tremaining: 18.7s\n",
      "325:\tlearn: 0.0004948\ttotal: 9s\tremaining: 18.6s\n",
      "326:\tlearn: 0.0004920\ttotal: 9.02s\tremaining: 18.6s\n",
      "327:\tlearn: 0.0004899\ttotal: 9.05s\tremaining: 18.5s\n",
      "328:\tlearn: 0.0004896\ttotal: 9.07s\tremaining: 18.5s\n",
      "329:\tlearn: 0.0004893\ttotal: 9.09s\tremaining: 18.5s\n",
      "330:\tlearn: 0.0004845\ttotal: 9.11s\tremaining: 18.4s\n",
      "331:\tlearn: 0.0004805\ttotal: 9.14s\tremaining: 18.4s\n",
      "332:\tlearn: 0.0004784\ttotal: 9.16s\tremaining: 18.3s\n",
      "333:\tlearn: 0.0004765\ttotal: 9.18s\tremaining: 18.3s\n",
      "334:\tlearn: 0.0004762\ttotal: 9.2s\tremaining: 18.3s\n",
      "335:\tlearn: 0.0004732\ttotal: 9.22s\tremaining: 18.2s\n",
      "336:\tlearn: 0.0004727\ttotal: 9.24s\tremaining: 18.2s\n",
      "337:\tlearn: 0.0004690\ttotal: 9.27s\tremaining: 18.2s\n",
      "338:\tlearn: 0.0004664\ttotal: 9.29s\tremaining: 18.1s\n",
      "339:\tlearn: 0.0004612\ttotal: 9.31s\tremaining: 18.1s\n",
      "340:\tlearn: 0.0004608\ttotal: 9.34s\tremaining: 18s\n",
      "341:\tlearn: 0.0004568\ttotal: 9.36s\tremaining: 18s\n",
      "342:\tlearn: 0.0004552\ttotal: 9.38s\tremaining: 18s\n",
      "343:\tlearn: 0.0004547\ttotal: 9.4s\tremaining: 17.9s\n",
      "344:\tlearn: 0.0004517\ttotal: 9.43s\tremaining: 17.9s\n",
      "345:\tlearn: 0.0004493\ttotal: 9.45s\tremaining: 17.9s\n",
      "346:\tlearn: 0.0004487\ttotal: 9.47s\tremaining: 17.8s\n",
      "347:\tlearn: 0.0004462\ttotal: 9.49s\tremaining: 17.8s\n",
      "348:\tlearn: 0.0004450\ttotal: 9.51s\tremaining: 17.7s\n",
      "349:\tlearn: 0.0004422\ttotal: 9.53s\tremaining: 17.7s\n",
      "350:\tlearn: 0.0004419\ttotal: 9.55s\tremaining: 17.7s\n",
      "351:\tlearn: 0.0004404\ttotal: 9.57s\tremaining: 17.6s\n",
      "352:\tlearn: 0.0004402\ttotal: 9.6s\tremaining: 17.6s\n",
      "353:\tlearn: 0.0004395\ttotal: 9.62s\tremaining: 17.5s\n",
      "354:\tlearn: 0.0004388\ttotal: 9.64s\tremaining: 17.5s\n",
      "355:\tlearn: 0.0004347\ttotal: 9.66s\tremaining: 17.5s\n",
      "356:\tlearn: 0.0004345\ttotal: 9.68s\tremaining: 17.4s\n",
      "357:\tlearn: 0.0004319\ttotal: 9.72s\tremaining: 17.4s\n",
      "358:\tlearn: 0.0004315\ttotal: 9.74s\tremaining: 17.4s\n",
      "359:\tlearn: 0.0004291\ttotal: 9.76s\tremaining: 17.4s\n",
      "360:\tlearn: 0.0004264\ttotal: 9.79s\tremaining: 17.3s\n",
      "361:\tlearn: 0.0004242\ttotal: 9.81s\tremaining: 17.3s\n",
      "362:\tlearn: 0.0004232\ttotal: 9.83s\tremaining: 17.3s\n",
      "363:\tlearn: 0.0004228\ttotal: 9.85s\tremaining: 17.2s\n",
      "364:\tlearn: 0.0004206\ttotal: 9.88s\tremaining: 17.2s\n",
      "365:\tlearn: 0.0004203\ttotal: 9.9s\tremaining: 17.1s\n",
      "366:\tlearn: 0.0004186\ttotal: 9.93s\tremaining: 17.1s\n",
      "367:\tlearn: 0.0004179\ttotal: 9.95s\tremaining: 17.1s\n",
      "368:\tlearn: 0.0004158\ttotal: 9.97s\tremaining: 17.1s\n",
      "369:\tlearn: 0.0004147\ttotal: 9.99s\tremaining: 17s\n",
      "370:\tlearn: 0.0004129\ttotal: 10s\tremaining: 17s\n",
      "371:\tlearn: 0.0004127\ttotal: 10s\tremaining: 17s\n",
      "372:\tlearn: 0.0004105\ttotal: 10.1s\tremaining: 16.9s\n",
      "373:\tlearn: 0.0004094\ttotal: 10.1s\tremaining: 16.9s\n",
      "374:\tlearn: 0.0004077\ttotal: 10.1s\tremaining: 16.9s\n",
      "375:\tlearn: 0.0004062\ttotal: 10.1s\tremaining: 16.8s\n",
      "376:\tlearn: 0.0004059\ttotal: 10.2s\tremaining: 16.8s\n",
      "377:\tlearn: 0.0004044\ttotal: 10.2s\tremaining: 16.8s\n",
      "378:\tlearn: 0.0004035\ttotal: 10.2s\tremaining: 16.7s\n",
      "379:\tlearn: 0.0004016\ttotal: 10.2s\tremaining: 16.7s\n",
      "380:\tlearn: 0.0004008\ttotal: 10.3s\tremaining: 16.7s\n",
      "381:\tlearn: 0.0004001\ttotal: 10.3s\tremaining: 16.6s\n",
      "382:\tlearn: 0.0003993\ttotal: 10.3s\tremaining: 16.6s\n",
      "383:\tlearn: 0.0003986\ttotal: 10.3s\tremaining: 16.6s\n",
      "384:\tlearn: 0.0003982\ttotal: 10.3s\tremaining: 16.5s\n",
      "385:\tlearn: 0.0003956\ttotal: 10.4s\tremaining: 16.5s\n",
      "386:\tlearn: 0.0003953\ttotal: 10.4s\tremaining: 16.5s\n",
      "387:\tlearn: 0.0003945\ttotal: 10.4s\tremaining: 16.4s\n",
      "388:\tlearn: 0.0003935\ttotal: 10.4s\tremaining: 16.4s\n",
      "389:\tlearn: 0.0003923\ttotal: 10.5s\tremaining: 16.3s\n",
      "390:\tlearn: 0.0003907\ttotal: 10.5s\tremaining: 16.3s\n",
      "391:\tlearn: 0.0003892\ttotal: 10.5s\tremaining: 16.3s\n",
      "392:\tlearn: 0.0003874\ttotal: 10.5s\tremaining: 16.2s\n",
      "393:\tlearn: 0.0003859\ttotal: 10.5s\tremaining: 16.2s\n",
      "394:\tlearn: 0.0003846\ttotal: 10.6s\tremaining: 16.2s\n",
      "395:\tlearn: 0.0003824\ttotal: 10.6s\tremaining: 16.1s\n",
      "396:\tlearn: 0.0003804\ttotal: 10.6s\tremaining: 16.1s\n",
      "397:\tlearn: 0.0003779\ttotal: 10.6s\tremaining: 16.1s\n",
      "398:\tlearn: 0.0003773\ttotal: 10.6s\tremaining: 16s\n",
      "399:\tlearn: 0.0003754\ttotal: 10.7s\tremaining: 16s\n",
      "400:\tlearn: 0.0003741\ttotal: 10.7s\tremaining: 16s\n",
      "401:\tlearn: 0.0003722\ttotal: 10.7s\tremaining: 15.9s\n",
      "402:\tlearn: 0.0003721\ttotal: 10.7s\tremaining: 15.9s\n",
      "403:\tlearn: 0.0003670\ttotal: 10.7s\tremaining: 15.9s\n",
      "404:\tlearn: 0.0003640\ttotal: 10.8s\tremaining: 15.8s\n",
      "405:\tlearn: 0.0003611\ttotal: 10.8s\tremaining: 15.8s\n",
      "406:\tlearn: 0.0003594\ttotal: 10.8s\tremaining: 15.8s\n",
      "407:\tlearn: 0.0003582\ttotal: 10.8s\tremaining: 15.7s\n",
      "408:\tlearn: 0.0003551\ttotal: 10.9s\tremaining: 15.7s\n",
      "409:\tlearn: 0.0003537\ttotal: 10.9s\tremaining: 15.7s\n",
      "410:\tlearn: 0.0003524\ttotal: 10.9s\tremaining: 15.6s\n",
      "411:\tlearn: 0.0003511\ttotal: 10.9s\tremaining: 15.6s\n",
      "412:\tlearn: 0.0003494\ttotal: 10.9s\tremaining: 15.6s\n",
      "413:\tlearn: 0.0003482\ttotal: 11s\tremaining: 15.5s\n",
      "414:\tlearn: 0.0003470\ttotal: 11s\tremaining: 15.5s\n",
      "415:\tlearn: 0.0003459\ttotal: 11s\tremaining: 15.5s\n",
      "416:\tlearn: 0.0003442\ttotal: 11s\tremaining: 15.4s\n",
      "417:\tlearn: 0.0003391\ttotal: 11s\tremaining: 15.4s\n",
      "418:\tlearn: 0.0003373\ttotal: 11.1s\tremaining: 15.3s\n",
      "419:\tlearn: 0.0003348\ttotal: 11.1s\tremaining: 15.3s\n",
      "420:\tlearn: 0.0003323\ttotal: 11.1s\tremaining: 15.3s\n",
      "421:\tlearn: 0.0003280\ttotal: 11.1s\tremaining: 15.3s\n",
      "422:\tlearn: 0.0003252\ttotal: 11.2s\tremaining: 15.2s\n",
      "423:\tlearn: 0.0003237\ttotal: 11.2s\tremaining: 15.2s\n",
      "424:\tlearn: 0.0003193\ttotal: 11.2s\tremaining: 15.2s\n",
      "425:\tlearn: 0.0003188\ttotal: 11.2s\tremaining: 15.1s\n",
      "426:\tlearn: 0.0003185\ttotal: 11.3s\tremaining: 15.1s\n",
      "427:\tlearn: 0.0003147\ttotal: 11.3s\tremaining: 15.1s\n",
      "428:\tlearn: 0.0003142\ttotal: 11.3s\tremaining: 15.1s\n",
      "429:\tlearn: 0.0003129\ttotal: 11.3s\tremaining: 15s\n",
      "430:\tlearn: 0.0003105\ttotal: 11.4s\tremaining: 15s\n",
      "431:\tlearn: 0.0003087\ttotal: 11.4s\tremaining: 15s\n",
      "432:\tlearn: 0.0003069\ttotal: 11.4s\tremaining: 14.9s\n",
      "433:\tlearn: 0.0003062\ttotal: 11.4s\tremaining: 14.9s\n",
      "434:\tlearn: 0.0003046\ttotal: 11.5s\tremaining: 14.9s\n",
      "435:\tlearn: 0.0003038\ttotal: 11.5s\tremaining: 14.9s\n",
      "436:\tlearn: 0.0003036\ttotal: 11.5s\tremaining: 14.8s\n",
      "437:\tlearn: 0.0003027\ttotal: 11.5s\tremaining: 14.8s\n",
      "438:\tlearn: 0.0003017\ttotal: 11.5s\tremaining: 14.8s\n",
      "439:\tlearn: 0.0002982\ttotal: 11.6s\tremaining: 14.7s\n",
      "440:\tlearn: 0.0002971\ttotal: 11.6s\tremaining: 14.7s\n",
      "441:\tlearn: 0.0002958\ttotal: 11.6s\tremaining: 14.7s\n",
      "442:\tlearn: 0.0002950\ttotal: 11.6s\tremaining: 14.6s\n",
      "443:\tlearn: 0.0002925\ttotal: 11.7s\tremaining: 14.6s\n",
      "444:\tlearn: 0.0002902\ttotal: 11.7s\tremaining: 14.6s\n",
      "445:\tlearn: 0.0002897\ttotal: 11.7s\tremaining: 14.5s\n",
      "446:\tlearn: 0.0002883\ttotal: 11.7s\tremaining: 14.5s\n",
      "447:\tlearn: 0.0002873\ttotal: 11.7s\tremaining: 14.5s\n",
      "448:\tlearn: 0.0002858\ttotal: 11.8s\tremaining: 14.4s\n",
      "449:\tlearn: 0.0002855\ttotal: 11.8s\tremaining: 14.4s\n",
      "450:\tlearn: 0.0002846\ttotal: 11.8s\tremaining: 14.4s\n",
      "451:\tlearn: 0.0002833\ttotal: 11.8s\tremaining: 14.3s\n",
      "452:\tlearn: 0.0002828\ttotal: 11.8s\tremaining: 14.3s\n",
      "453:\tlearn: 0.0002816\ttotal: 11.9s\tremaining: 14.3s\n",
      "454:\tlearn: 0.0002802\ttotal: 11.9s\tremaining: 14.2s\n",
      "455:\tlearn: 0.0002799\ttotal: 11.9s\tremaining: 14.2s\n",
      "456:\tlearn: 0.0002787\ttotal: 11.9s\tremaining: 14.2s\n",
      "457:\tlearn: 0.0002782\ttotal: 12s\tremaining: 14.1s\n",
      "458:\tlearn: 0.0002781\ttotal: 12s\tremaining: 14.1s\n",
      "459:\tlearn: 0.0002756\ttotal: 12s\tremaining: 14.1s\n",
      "460:\tlearn: 0.0002755\ttotal: 12s\tremaining: 14.1s\n",
      "461:\tlearn: 0.0002742\ttotal: 12s\tremaining: 14s\n",
      "462:\tlearn: 0.0002731\ttotal: 12.1s\tremaining: 14s\n",
      "463:\tlearn: 0.0002718\ttotal: 12.1s\tremaining: 14s\n",
      "464:\tlearn: 0.0002715\ttotal: 12.1s\tremaining: 13.9s\n",
      "465:\tlearn: 0.0002706\ttotal: 12.1s\tremaining: 13.9s\n",
      "466:\tlearn: 0.0002697\ttotal: 12.1s\tremaining: 13.9s\n",
      "467:\tlearn: 0.0002689\ttotal: 12.2s\tremaining: 13.8s\n",
      "468:\tlearn: 0.0002685\ttotal: 12.2s\tremaining: 13.8s\n",
      "469:\tlearn: 0.0002672\ttotal: 12.2s\tremaining: 13.8s\n",
      "470:\tlearn: 0.0002665\ttotal: 12.2s\tremaining: 13.7s\n",
      "471:\tlearn: 0.0002650\ttotal: 12.2s\tremaining: 13.7s\n",
      "472:\tlearn: 0.0002637\ttotal: 12.3s\tremaining: 13.7s\n",
      "473:\tlearn: 0.0002607\ttotal: 12.3s\tremaining: 13.6s\n",
      "474:\tlearn: 0.0002603\ttotal: 12.3s\tremaining: 13.6s\n",
      "475:\tlearn: 0.0002591\ttotal: 12.3s\tremaining: 13.6s\n",
      "476:\tlearn: 0.0002579\ttotal: 12.4s\tremaining: 13.5s\n",
      "477:\tlearn: 0.0002578\ttotal: 12.4s\tremaining: 13.5s\n",
      "478:\tlearn: 0.0002555\ttotal: 12.4s\tremaining: 13.5s\n",
      "479:\tlearn: 0.0002546\ttotal: 12.4s\tremaining: 13.5s\n",
      "480:\tlearn: 0.0002535\ttotal: 12.4s\tremaining: 13.4s\n",
      "481:\tlearn: 0.0002534\ttotal: 12.5s\tremaining: 13.4s\n",
      "482:\tlearn: 0.0002533\ttotal: 12.5s\tremaining: 13.4s\n",
      "483:\tlearn: 0.0002530\ttotal: 12.5s\tremaining: 13.3s\n",
      "484:\tlearn: 0.0002514\ttotal: 12.5s\tremaining: 13.3s\n",
      "485:\tlearn: 0.0002507\ttotal: 12.5s\tremaining: 13.3s\n",
      "486:\tlearn: 0.0002504\ttotal: 12.6s\tremaining: 13.2s\n",
      "487:\tlearn: 0.0002504\ttotal: 12.6s\tremaining: 13.2s\n",
      "488:\tlearn: 0.0002497\ttotal: 12.6s\tremaining: 13.2s\n",
      "489:\tlearn: 0.0002496\ttotal: 12.6s\tremaining: 13.1s\n",
      "490:\tlearn: 0.0002485\ttotal: 12.7s\tremaining: 13.1s\n",
      "491:\tlearn: 0.0002478\ttotal: 12.7s\tremaining: 13.1s\n",
      "492:\tlearn: 0.0002475\ttotal: 12.7s\tremaining: 13.1s\n",
      "493:\tlearn: 0.0002474\ttotal: 12.7s\tremaining: 13s\n",
      "494:\tlearn: 0.0002463\ttotal: 12.7s\tremaining: 13s\n",
      "495:\tlearn: 0.0002461\ttotal: 12.8s\tremaining: 13s\n",
      "496:\tlearn: 0.0002457\ttotal: 12.8s\tremaining: 12.9s\n",
      "497:\tlearn: 0.0002452\ttotal: 12.8s\tremaining: 12.9s\n",
      "498:\tlearn: 0.0002439\ttotal: 12.8s\tremaining: 12.9s\n",
      "499:\tlearn: 0.0002426\ttotal: 12.8s\tremaining: 12.8s\n",
      "500:\tlearn: 0.0002407\ttotal: 12.9s\tremaining: 12.8s\n",
      "501:\tlearn: 0.0002397\ttotal: 12.9s\tremaining: 12.8s\n",
      "502:\tlearn: 0.0002379\ttotal: 12.9s\tremaining: 12.8s\n",
      "503:\tlearn: 0.0002367\ttotal: 12.9s\tremaining: 12.7s\n",
      "504:\tlearn: 0.0002363\ttotal: 13s\tremaining: 12.7s\n",
      "505:\tlearn: 0.0002352\ttotal: 13s\tremaining: 12.7s\n",
      "506:\tlearn: 0.0002349\ttotal: 13s\tremaining: 12.6s\n",
      "507:\tlearn: 0.0002333\ttotal: 13s\tremaining: 12.6s\n",
      "508:\tlearn: 0.0002317\ttotal: 13s\tremaining: 12.6s\n",
      "509:\tlearn: 0.0002309\ttotal: 13.1s\tremaining: 12.5s\n",
      "510:\tlearn: 0.0002298\ttotal: 13.1s\tremaining: 12.5s\n",
      "511:\tlearn: 0.0002297\ttotal: 13.1s\tremaining: 12.5s\n",
      "512:\tlearn: 0.0002285\ttotal: 13.1s\tremaining: 12.5s\n",
      "513:\tlearn: 0.0002255\ttotal: 13.1s\tremaining: 12.4s\n",
      "514:\tlearn: 0.0002246\ttotal: 13.2s\tremaining: 12.4s\n",
      "515:\tlearn: 0.0002242\ttotal: 13.2s\tremaining: 12.4s\n",
      "516:\tlearn: 0.0002238\ttotal: 13.2s\tremaining: 12.3s\n",
      "517:\tlearn: 0.0002216\ttotal: 13.2s\tremaining: 12.3s\n",
      "518:\tlearn: 0.0002209\ttotal: 13.2s\tremaining: 12.3s\n",
      "519:\tlearn: 0.0002208\ttotal: 13.3s\tremaining: 12.2s\n",
      "520:\tlearn: 0.0002206\ttotal: 13.3s\tremaining: 12.2s\n",
      "521:\tlearn: 0.0002199\ttotal: 13.3s\tremaining: 12.2s\n",
      "522:\tlearn: 0.0002191\ttotal: 13.3s\tremaining: 12.1s\n",
      "523:\tlearn: 0.0002189\ttotal: 13.3s\tremaining: 12.1s\n",
      "524:\tlearn: 0.0002174\ttotal: 13.4s\tremaining: 12.1s\n",
      "525:\tlearn: 0.0002173\ttotal: 13.4s\tremaining: 12.1s\n",
      "526:\tlearn: 0.0002168\ttotal: 13.4s\tremaining: 12s\n",
      "527:\tlearn: 0.0002166\ttotal: 13.4s\tremaining: 12s\n",
      "528:\tlearn: 0.0002149\ttotal: 13.4s\tremaining: 12s\n",
      "529:\tlearn: 0.0002148\ttotal: 13.5s\tremaining: 11.9s\n",
      "530:\tlearn: 0.0002146\ttotal: 13.5s\tremaining: 11.9s\n",
      "531:\tlearn: 0.0002136\ttotal: 13.5s\tremaining: 11.9s\n",
      "532:\tlearn: 0.0002132\ttotal: 13.5s\tremaining: 11.9s\n",
      "533:\tlearn: 0.0002129\ttotal: 13.6s\tremaining: 11.8s\n",
      "534:\tlearn: 0.0002126\ttotal: 13.6s\tremaining: 11.8s\n",
      "535:\tlearn: 0.0002111\ttotal: 13.6s\tremaining: 11.8s\n",
      "536:\tlearn: 0.0002106\ttotal: 13.6s\tremaining: 11.7s\n",
      "537:\tlearn: 0.0002099\ttotal: 13.6s\tremaining: 11.7s\n",
      "538:\tlearn: 0.0002090\ttotal: 13.7s\tremaining: 11.7s\n",
      "539:\tlearn: 0.0002089\ttotal: 13.7s\tremaining: 11.7s\n",
      "540:\tlearn: 0.0002086\ttotal: 13.7s\tremaining: 11.6s\n",
      "541:\tlearn: 0.0002080\ttotal: 13.7s\tremaining: 11.6s\n",
      "542:\tlearn: 0.0002072\ttotal: 13.7s\tremaining: 11.6s\n",
      "543:\tlearn: 0.0002070\ttotal: 13.8s\tremaining: 11.5s\n",
      "544:\tlearn: 0.0002070\ttotal: 13.8s\tremaining: 11.5s\n",
      "545:\tlearn: 0.0002067\ttotal: 13.8s\tremaining: 11.5s\n",
      "546:\tlearn: 0.0002066\ttotal: 13.8s\tremaining: 11.5s\n",
      "547:\tlearn: 0.0002064\ttotal: 13.8s\tremaining: 11.4s\n",
      "548:\tlearn: 0.0002063\ttotal: 13.9s\tremaining: 11.4s\n",
      "549:\tlearn: 0.0002053\ttotal: 13.9s\tremaining: 11.4s\n",
      "550:\tlearn: 0.0002043\ttotal: 13.9s\tremaining: 11.3s\n",
      "551:\tlearn: 0.0002038\ttotal: 13.9s\tremaining: 11.3s\n",
      "552:\tlearn: 0.0002027\ttotal: 14s\tremaining: 11.3s\n",
      "553:\tlearn: 0.0002015\ttotal: 14s\tremaining: 11.3s\n",
      "554:\tlearn: 0.0002006\ttotal: 14s\tremaining: 11.2s\n",
      "555:\tlearn: 0.0001999\ttotal: 14s\tremaining: 11.2s\n",
      "556:\tlearn: 0.0001993\ttotal: 14s\tremaining: 11.2s\n",
      "557:\tlearn: 0.0001988\ttotal: 14.1s\tremaining: 11.1s\n",
      "558:\tlearn: 0.0001985\ttotal: 14.1s\tremaining: 11.1s\n",
      "559:\tlearn: 0.0001973\ttotal: 14.1s\tremaining: 11.1s\n",
      "560:\tlearn: 0.0001970\ttotal: 14.1s\tremaining: 11.1s\n",
      "561:\tlearn: 0.0001962\ttotal: 14.1s\tremaining: 11s\n",
      "562:\tlearn: 0.0001959\ttotal: 14.2s\tremaining: 11s\n",
      "563:\tlearn: 0.0001947\ttotal: 14.2s\tremaining: 11s\n",
      "564:\tlearn: 0.0001937\ttotal: 14.2s\tremaining: 10.9s\n",
      "565:\tlearn: 0.0001934\ttotal: 14.2s\tremaining: 10.9s\n",
      "566:\tlearn: 0.0001932\ttotal: 14.3s\tremaining: 10.9s\n",
      "567:\tlearn: 0.0001925\ttotal: 14.3s\tremaining: 10.9s\n",
      "568:\tlearn: 0.0001921\ttotal: 14.3s\tremaining: 10.8s\n",
      "569:\tlearn: 0.0001914\ttotal: 14.3s\tremaining: 10.8s\n",
      "570:\tlearn: 0.0001912\ttotal: 14.3s\tremaining: 10.8s\n",
      "571:\tlearn: 0.0001909\ttotal: 14.4s\tremaining: 10.7s\n",
      "572:\tlearn: 0.0001906\ttotal: 14.4s\tremaining: 10.7s\n",
      "573:\tlearn: 0.0001903\ttotal: 14.4s\tremaining: 10.7s\n",
      "574:\tlearn: 0.0001899\ttotal: 14.4s\tremaining: 10.7s\n",
      "575:\tlearn: 0.0001897\ttotal: 14.4s\tremaining: 10.6s\n",
      "576:\tlearn: 0.0001896\ttotal: 14.5s\tremaining: 10.6s\n",
      "577:\tlearn: 0.0001895\ttotal: 14.5s\tremaining: 10.6s\n",
      "578:\tlearn: 0.0001881\ttotal: 14.5s\tremaining: 10.6s\n",
      "579:\tlearn: 0.0001871\ttotal: 14.5s\tremaining: 10.5s\n",
      "580:\tlearn: 0.0001863\ttotal: 14.6s\tremaining: 10.5s\n",
      "581:\tlearn: 0.0001860\ttotal: 14.6s\tremaining: 10.5s\n",
      "582:\tlearn: 0.0001855\ttotal: 14.6s\tremaining: 10.5s\n",
      "583:\tlearn: 0.0001854\ttotal: 14.7s\tremaining: 10.4s\n",
      "584:\tlearn: 0.0001853\ttotal: 14.7s\tremaining: 10.4s\n",
      "585:\tlearn: 0.0001848\ttotal: 14.7s\tremaining: 10.4s\n",
      "586:\tlearn: 0.0001837\ttotal: 14.7s\tremaining: 10.4s\n",
      "587:\tlearn: 0.0001833\ttotal: 14.8s\tremaining: 10.3s\n",
      "588:\tlearn: 0.0001822\ttotal: 14.8s\tremaining: 10.3s\n",
      "589:\tlearn: 0.0001816\ttotal: 14.8s\tremaining: 10.3s\n",
      "590:\tlearn: 0.0001813\ttotal: 14.8s\tremaining: 10.3s\n",
      "591:\tlearn: 0.0001809\ttotal: 14.9s\tremaining: 10.2s\n",
      "592:\tlearn: 0.0001808\ttotal: 14.9s\tremaining: 10.2s\n",
      "593:\tlearn: 0.0001807\ttotal: 14.9s\tremaining: 10.2s\n",
      "594:\tlearn: 0.0001806\ttotal: 14.9s\tremaining: 10.2s\n",
      "595:\tlearn: 0.0001803\ttotal: 14.9s\tremaining: 10.1s\n",
      "596:\tlearn: 0.0001800\ttotal: 15s\tremaining: 10.1s\n",
      "597:\tlearn: 0.0001790\ttotal: 15s\tremaining: 10.1s\n",
      "598:\tlearn: 0.0001780\ttotal: 15s\tremaining: 10s\n",
      "599:\tlearn: 0.0001771\ttotal: 15s\tremaining: 10s\n",
      "600:\tlearn: 0.0001759\ttotal: 15s\tremaining: 9.99s\n",
      "601:\tlearn: 0.0001755\ttotal: 15.1s\tremaining: 9.96s\n",
      "602:\tlearn: 0.0001747\ttotal: 15.1s\tremaining: 9.93s\n",
      "603:\tlearn: 0.0001733\ttotal: 15.1s\tremaining: 9.91s\n",
      "604:\tlearn: 0.0001730\ttotal: 15.1s\tremaining: 9.88s\n",
      "605:\tlearn: 0.0001724\ttotal: 15.2s\tremaining: 9.85s\n",
      "606:\tlearn: 0.0001723\ttotal: 15.2s\tremaining: 9.82s\n",
      "607:\tlearn: 0.0001718\ttotal: 15.2s\tremaining: 9.79s\n",
      "608:\tlearn: 0.0001707\ttotal: 15.2s\tremaining: 9.77s\n",
      "609:\tlearn: 0.0001704\ttotal: 15.2s\tremaining: 9.74s\n",
      "610:\tlearn: 0.0001696\ttotal: 15.3s\tremaining: 9.71s\n",
      "611:\tlearn: 0.0001686\ttotal: 15.3s\tremaining: 9.69s\n",
      "612:\tlearn: 0.0001680\ttotal: 15.3s\tremaining: 9.66s\n",
      "613:\tlearn: 0.0001669\ttotal: 15.3s\tremaining: 9.63s\n",
      "614:\tlearn: 0.0001668\ttotal: 15.3s\tremaining: 9.61s\n",
      "615:\tlearn: 0.0001664\ttotal: 15.4s\tremaining: 9.58s\n",
      "616:\tlearn: 0.0001657\ttotal: 15.4s\tremaining: 9.55s\n",
      "617:\tlearn: 0.0001654\ttotal: 15.4s\tremaining: 9.52s\n",
      "618:\tlearn: 0.0001653\ttotal: 15.4s\tremaining: 9.5s\n",
      "619:\tlearn: 0.0001647\ttotal: 15.4s\tremaining: 9.47s\n",
      "620:\tlearn: 0.0001642\ttotal: 15.5s\tremaining: 9.44s\n",
      "621:\tlearn: 0.0001632\ttotal: 15.5s\tremaining: 9.42s\n",
      "622:\tlearn: 0.0001629\ttotal: 15.5s\tremaining: 9.39s\n",
      "623:\tlearn: 0.0001627\ttotal: 15.5s\tremaining: 9.36s\n",
      "624:\tlearn: 0.0001615\ttotal: 15.6s\tremaining: 9.34s\n",
      "625:\tlearn: 0.0001611\ttotal: 15.6s\tremaining: 9.31s\n",
      "626:\tlearn: 0.0001607\ttotal: 15.6s\tremaining: 9.28s\n",
      "627:\tlearn: 0.0001599\ttotal: 15.6s\tremaining: 9.26s\n",
      "628:\tlearn: 0.0001597\ttotal: 15.6s\tremaining: 9.23s\n",
      "629:\tlearn: 0.0001589\ttotal: 15.7s\tremaining: 9.21s\n",
      "630:\tlearn: 0.0001583\ttotal: 15.7s\tremaining: 9.18s\n",
      "631:\tlearn: 0.0001574\ttotal: 15.7s\tremaining: 9.15s\n",
      "632:\tlearn: 0.0001572\ttotal: 15.7s\tremaining: 9.13s\n",
      "633:\tlearn: 0.0001566\ttotal: 15.8s\tremaining: 9.1s\n",
      "634:\tlearn: 0.0001563\ttotal: 15.8s\tremaining: 9.07s\n",
      "635:\tlearn: 0.0001562\ttotal: 15.8s\tremaining: 9.05s\n",
      "636:\tlearn: 0.0001559\ttotal: 15.8s\tremaining: 9.02s\n",
      "637:\tlearn: 0.0001556\ttotal: 15.8s\tremaining: 8.99s\n",
      "638:\tlearn: 0.0001550\ttotal: 15.9s\tremaining: 8.97s\n",
      "639:\tlearn: 0.0001547\ttotal: 15.9s\tremaining: 8.94s\n",
      "640:\tlearn: 0.0001537\ttotal: 15.9s\tremaining: 8.92s\n",
      "641:\tlearn: 0.0001533\ttotal: 15.9s\tremaining: 8.89s\n",
      "642:\tlearn: 0.0001532\ttotal: 16s\tremaining: 8.86s\n",
      "643:\tlearn: 0.0001530\ttotal: 16s\tremaining: 8.84s\n",
      "644:\tlearn: 0.0001529\ttotal: 16s\tremaining: 8.81s\n",
      "645:\tlearn: 0.0001528\ttotal: 16s\tremaining: 8.78s\n",
      "646:\tlearn: 0.0001523\ttotal: 16.1s\tremaining: 8.76s\n",
      "647:\tlearn: 0.0001518\ttotal: 16.1s\tremaining: 8.73s\n",
      "648:\tlearn: 0.0001513\ttotal: 16.1s\tremaining: 8.71s\n",
      "649:\tlearn: 0.0001507\ttotal: 16.1s\tremaining: 8.69s\n",
      "650:\tlearn: 0.0001505\ttotal: 16.2s\tremaining: 8.66s\n",
      "651:\tlearn: 0.0001499\ttotal: 16.2s\tremaining: 8.63s\n",
      "652:\tlearn: 0.0001498\ttotal: 16.2s\tremaining: 8.61s\n",
      "653:\tlearn: 0.0001497\ttotal: 16.2s\tremaining: 8.58s\n",
      "654:\tlearn: 0.0001494\ttotal: 16.2s\tremaining: 8.55s\n",
      "655:\tlearn: 0.0001486\ttotal: 16.3s\tremaining: 8.53s\n",
      "656:\tlearn: 0.0001483\ttotal: 16.3s\tremaining: 8.5s\n",
      "657:\tlearn: 0.0001482\ttotal: 16.3s\tremaining: 8.47s\n",
      "658:\tlearn: 0.0001480\ttotal: 16.3s\tremaining: 8.44s\n",
      "659:\tlearn: 0.0001464\ttotal: 16.3s\tremaining: 8.42s\n",
      "660:\tlearn: 0.0001463\ttotal: 16.4s\tremaining: 8.39s\n",
      "661:\tlearn: 0.0001462\ttotal: 16.4s\tremaining: 8.37s\n",
      "662:\tlearn: 0.0001460\ttotal: 16.4s\tremaining: 8.34s\n",
      "663:\tlearn: 0.0001459\ttotal: 16.4s\tremaining: 8.31s\n",
      "664:\tlearn: 0.0001459\ttotal: 16.5s\tremaining: 8.29s\n",
      "665:\tlearn: 0.0001453\ttotal: 16.5s\tremaining: 8.26s\n",
      "666:\tlearn: 0.0001446\ttotal: 16.5s\tremaining: 8.23s\n",
      "667:\tlearn: 0.0001446\ttotal: 16.5s\tremaining: 8.21s\n",
      "668:\tlearn: 0.0001437\ttotal: 16.5s\tremaining: 8.18s\n",
      "669:\tlearn: 0.0001436\ttotal: 16.6s\tremaining: 8.16s\n",
      "670:\tlearn: 0.0001435\ttotal: 16.6s\tremaining: 8.13s\n",
      "671:\tlearn: 0.0001432\ttotal: 16.6s\tremaining: 8.1s\n",
      "672:\tlearn: 0.0001427\ttotal: 16.6s\tremaining: 8.08s\n",
      "673:\tlearn: 0.0001426\ttotal: 16.6s\tremaining: 8.05s\n",
      "674:\tlearn: 0.0001425\ttotal: 16.7s\tremaining: 8.02s\n",
      "675:\tlearn: 0.0001421\ttotal: 16.7s\tremaining: 8s\n",
      "676:\tlearn: 0.0001417\ttotal: 16.7s\tremaining: 7.97s\n",
      "677:\tlearn: 0.0001417\ttotal: 16.7s\tremaining: 7.94s\n",
      "678:\tlearn: 0.0001411\ttotal: 16.7s\tremaining: 7.92s\n",
      "679:\tlearn: 0.0001410\ttotal: 16.8s\tremaining: 7.89s\n",
      "680:\tlearn: 0.0001405\ttotal: 16.8s\tremaining: 7.87s\n",
      "681:\tlearn: 0.0001401\ttotal: 16.8s\tremaining: 7.84s\n",
      "682:\tlearn: 0.0001399\ttotal: 16.8s\tremaining: 7.81s\n",
      "683:\tlearn: 0.0001397\ttotal: 16.9s\tremaining: 7.79s\n",
      "684:\tlearn: 0.0001390\ttotal: 16.9s\tremaining: 7.76s\n",
      "685:\tlearn: 0.0001389\ttotal: 16.9s\tremaining: 7.74s\n",
      "686:\tlearn: 0.0001387\ttotal: 16.9s\tremaining: 7.71s\n",
      "687:\tlearn: 0.0001381\ttotal: 17s\tremaining: 7.69s\n",
      "688:\tlearn: 0.0001372\ttotal: 17s\tremaining: 7.66s\n",
      "689:\tlearn: 0.0001371\ttotal: 17s\tremaining: 7.64s\n",
      "690:\tlearn: 0.0001370\ttotal: 17s\tremaining: 7.61s\n",
      "691:\tlearn: 0.0001370\ttotal: 17s\tremaining: 7.58s\n",
      "692:\tlearn: 0.0001359\ttotal: 17.1s\tremaining: 7.56s\n",
      "693:\tlearn: 0.0001359\ttotal: 17.1s\tremaining: 7.53s\n",
      "694:\tlearn: 0.0001357\ttotal: 17.1s\tremaining: 7.51s\n",
      "695:\tlearn: 0.0001353\ttotal: 17.1s\tremaining: 7.48s\n",
      "696:\tlearn: 0.0001350\ttotal: 17.2s\tremaining: 7.46s\n",
      "697:\tlearn: 0.0001349\ttotal: 17.2s\tremaining: 7.43s\n",
      "698:\tlearn: 0.0001346\ttotal: 17.2s\tremaining: 7.41s\n",
      "699:\tlearn: 0.0001345\ttotal: 17.2s\tremaining: 7.38s\n",
      "700:\tlearn: 0.0001344\ttotal: 17.3s\tremaining: 7.36s\n",
      "701:\tlearn: 0.0001340\ttotal: 17.3s\tremaining: 7.33s\n",
      "702:\tlearn: 0.0001338\ttotal: 17.3s\tremaining: 7.31s\n",
      "703:\tlearn: 0.0001333\ttotal: 17.3s\tremaining: 7.28s\n",
      "704:\tlearn: 0.0001331\ttotal: 17.3s\tremaining: 7.25s\n",
      "705:\tlearn: 0.0001326\ttotal: 17.4s\tremaining: 7.23s\n",
      "706:\tlearn: 0.0001324\ttotal: 17.4s\tremaining: 7.21s\n",
      "707:\tlearn: 0.0001320\ttotal: 17.4s\tremaining: 7.18s\n",
      "708:\tlearn: 0.0001318\ttotal: 17.4s\tremaining: 7.15s\n",
      "709:\tlearn: 0.0001317\ttotal: 17.5s\tremaining: 7.13s\n",
      "710:\tlearn: 0.0001313\ttotal: 17.5s\tremaining: 7.1s\n",
      "711:\tlearn: 0.0001311\ttotal: 17.5s\tremaining: 7.08s\n",
      "712:\tlearn: 0.0001308\ttotal: 17.5s\tremaining: 7.05s\n",
      "713:\tlearn: 0.0001304\ttotal: 17.5s\tremaining: 7.03s\n",
      "714:\tlearn: 0.0001300\ttotal: 17.6s\tremaining: 7s\n",
      "715:\tlearn: 0.0001296\ttotal: 17.6s\tremaining: 6.97s\n",
      "716:\tlearn: 0.0001293\ttotal: 17.6s\tremaining: 6.95s\n",
      "717:\tlearn: 0.0001285\ttotal: 17.6s\tremaining: 6.92s\n",
      "718:\tlearn: 0.0001281\ttotal: 17.7s\tremaining: 6.9s\n",
      "719:\tlearn: 0.0001279\ttotal: 17.7s\tremaining: 6.87s\n",
      "720:\tlearn: 0.0001276\ttotal: 17.7s\tremaining: 6.85s\n",
      "721:\tlearn: 0.0001274\ttotal: 17.7s\tremaining: 6.82s\n",
      "722:\tlearn: 0.0001274\ttotal: 17.7s\tremaining: 6.8s\n",
      "723:\tlearn: 0.0001272\ttotal: 17.8s\tremaining: 6.77s\n",
      "724:\tlearn: 0.0001270\ttotal: 17.8s\tremaining: 6.75s\n",
      "725:\tlearn: 0.0001264\ttotal: 17.8s\tremaining: 6.72s\n",
      "726:\tlearn: 0.0001258\ttotal: 17.8s\tremaining: 6.69s\n",
      "727:\tlearn: 0.0001256\ttotal: 17.8s\tremaining: 6.67s\n",
      "728:\tlearn: 0.0001255\ttotal: 17.9s\tremaining: 6.64s\n",
      "729:\tlearn: 0.0001248\ttotal: 17.9s\tremaining: 6.62s\n",
      "730:\tlearn: 0.0001247\ttotal: 17.9s\tremaining: 6.59s\n",
      "731:\tlearn: 0.0001244\ttotal: 17.9s\tremaining: 6.57s\n",
      "732:\tlearn: 0.0001244\ttotal: 18s\tremaining: 6.54s\n",
      "733:\tlearn: 0.0001243\ttotal: 18s\tremaining: 6.51s\n",
      "734:\tlearn: 0.0001241\ttotal: 18s\tremaining: 6.49s\n",
      "735:\tlearn: 0.0001240\ttotal: 18s\tremaining: 6.46s\n",
      "736:\tlearn: 0.0001237\ttotal: 18s\tremaining: 6.44s\n",
      "737:\tlearn: 0.0001232\ttotal: 18.1s\tremaining: 6.41s\n",
      "738:\tlearn: 0.0001227\ttotal: 18.1s\tremaining: 6.39s\n",
      "739:\tlearn: 0.0001225\ttotal: 18.1s\tremaining: 6.36s\n",
      "740:\tlearn: 0.0001223\ttotal: 18.1s\tremaining: 6.34s\n",
      "741:\tlearn: 0.0001221\ttotal: 18.2s\tremaining: 6.31s\n",
      "742:\tlearn: 0.0001221\ttotal: 18.2s\tremaining: 6.29s\n",
      "743:\tlearn: 0.0001216\ttotal: 18.2s\tremaining: 6.26s\n",
      "744:\tlearn: 0.0001213\ttotal: 18.2s\tremaining: 6.24s\n",
      "745:\tlearn: 0.0001209\ttotal: 18.3s\tremaining: 6.21s\n",
      "746:\tlearn: 0.0001206\ttotal: 18.3s\tremaining: 6.19s\n",
      "747:\tlearn: 0.0001204\ttotal: 18.3s\tremaining: 6.16s\n",
      "748:\tlearn: 0.0001200\ttotal: 18.3s\tremaining: 6.14s\n",
      "749:\tlearn: 0.0001198\ttotal: 18.3s\tremaining: 6.11s\n",
      "750:\tlearn: 0.0001197\ttotal: 18.4s\tremaining: 6.09s\n",
      "751:\tlearn: 0.0001195\ttotal: 18.4s\tremaining: 6.06s\n",
      "752:\tlearn: 0.0001193\ttotal: 18.4s\tremaining: 6.04s\n",
      "753:\tlearn: 0.0001190\ttotal: 18.4s\tremaining: 6.01s\n",
      "754:\tlearn: 0.0001189\ttotal: 18.4s\tremaining: 5.99s\n",
      "755:\tlearn: 0.0001189\ttotal: 18.5s\tremaining: 5.96s\n",
      "756:\tlearn: 0.0001187\ttotal: 18.5s\tremaining: 5.93s\n",
      "757:\tlearn: 0.0001180\ttotal: 18.5s\tremaining: 5.91s\n",
      "758:\tlearn: 0.0001180\ttotal: 18.5s\tremaining: 5.88s\n",
      "759:\tlearn: 0.0001178\ttotal: 18.6s\tremaining: 5.86s\n",
      "760:\tlearn: 0.0001175\ttotal: 18.6s\tremaining: 5.83s\n",
      "761:\tlearn: 0.0001169\ttotal: 18.6s\tremaining: 5.81s\n",
      "762:\tlearn: 0.0001168\ttotal: 18.6s\tremaining: 5.78s\n",
      "763:\tlearn: 0.0001166\ttotal: 18.6s\tremaining: 5.76s\n",
      "764:\tlearn: 0.0001164\ttotal: 18.7s\tremaining: 5.73s\n",
      "765:\tlearn: 0.0001162\ttotal: 18.7s\tremaining: 5.71s\n",
      "766:\tlearn: 0.0001159\ttotal: 18.7s\tremaining: 5.68s\n",
      "767:\tlearn: 0.0001155\ttotal: 18.7s\tremaining: 5.66s\n",
      "768:\tlearn: 0.0001152\ttotal: 18.8s\tremaining: 5.63s\n",
      "769:\tlearn: 0.0001150\ttotal: 18.8s\tremaining: 5.61s\n",
      "770:\tlearn: 0.0001150\ttotal: 18.8s\tremaining: 5.59s\n",
      "771:\tlearn: 0.0001147\ttotal: 18.8s\tremaining: 5.56s\n",
      "772:\tlearn: 0.0001146\ttotal: 18.9s\tremaining: 5.54s\n",
      "773:\tlearn: 0.0001145\ttotal: 18.9s\tremaining: 5.51s\n",
      "774:\tlearn: 0.0001142\ttotal: 18.9s\tremaining: 5.49s\n",
      "775:\tlearn: 0.0001139\ttotal: 18.9s\tremaining: 5.47s\n",
      "776:\tlearn: 0.0001132\ttotal: 19s\tremaining: 5.44s\n",
      "777:\tlearn: 0.0001131\ttotal: 19s\tremaining: 5.42s\n",
      "778:\tlearn: 0.0001129\ttotal: 19s\tremaining: 5.4s\n",
      "779:\tlearn: 0.0001124\ttotal: 19s\tremaining: 5.37s\n",
      "780:\tlearn: 0.0001123\ttotal: 19.1s\tremaining: 5.35s\n",
      "781:\tlearn: 0.0001121\ttotal: 19.1s\tremaining: 5.32s\n",
      "782:\tlearn: 0.0001120\ttotal: 19.1s\tremaining: 5.3s\n",
      "783:\tlearn: 0.0001117\ttotal: 19.1s\tremaining: 5.28s\n",
      "784:\tlearn: 0.0001116\ttotal: 19.2s\tremaining: 5.25s\n",
      "785:\tlearn: 0.0001115\ttotal: 19.2s\tremaining: 5.23s\n",
      "786:\tlearn: 0.0001112\ttotal: 19.2s\tremaining: 5.2s\n",
      "787:\tlearn: 0.0001110\ttotal: 19.3s\tremaining: 5.18s\n",
      "788:\tlearn: 0.0001106\ttotal: 19.3s\tremaining: 5.15s\n",
      "789:\tlearn: 0.0001102\ttotal: 19.3s\tremaining: 5.13s\n",
      "790:\tlearn: 0.0001100\ttotal: 19.3s\tremaining: 5.11s\n",
      "791:\tlearn: 0.0001100\ttotal: 19.4s\tremaining: 5.08s\n",
      "792:\tlearn: 0.0001099\ttotal: 19.4s\tremaining: 5.06s\n",
      "793:\tlearn: 0.0001097\ttotal: 19.4s\tremaining: 5.04s\n",
      "794:\tlearn: 0.0001096\ttotal: 19.4s\tremaining: 5.01s\n",
      "795:\tlearn: 0.0001094\ttotal: 19.5s\tremaining: 4.99s\n",
      "796:\tlearn: 0.0001093\ttotal: 19.5s\tremaining: 4.96s\n",
      "797:\tlearn: 0.0001092\ttotal: 19.5s\tremaining: 4.94s\n",
      "798:\tlearn: 0.0001091\ttotal: 19.5s\tremaining: 4.92s\n",
      "799:\tlearn: 0.0001088\ttotal: 19.6s\tremaining: 4.89s\n",
      "800:\tlearn: 0.0001087\ttotal: 19.6s\tremaining: 4.87s\n",
      "801:\tlearn: 0.0001084\ttotal: 19.6s\tremaining: 4.84s\n",
      "802:\tlearn: 0.0001083\ttotal: 19.7s\tremaining: 4.82s\n",
      "803:\tlearn: 0.0001081\ttotal: 19.7s\tremaining: 4.8s\n",
      "804:\tlearn: 0.0001078\ttotal: 19.7s\tremaining: 4.77s\n",
      "805:\tlearn: 0.0001072\ttotal: 19.7s\tremaining: 4.75s\n",
      "806:\tlearn: 0.0001072\ttotal: 19.8s\tremaining: 4.72s\n",
      "807:\tlearn: 0.0001070\ttotal: 19.8s\tremaining: 4.7s\n",
      "808:\tlearn: 0.0001069\ttotal: 19.8s\tremaining: 4.67s\n",
      "809:\tlearn: 0.0001067\ttotal: 19.8s\tremaining: 4.65s\n",
      "810:\tlearn: 0.0001062\ttotal: 19.8s\tremaining: 4.62s\n",
      "811:\tlearn: 0.0001061\ttotal: 19.9s\tremaining: 4.6s\n",
      "812:\tlearn: 0.0001058\ttotal: 19.9s\tremaining: 4.58s\n",
      "813:\tlearn: 0.0001056\ttotal: 19.9s\tremaining: 4.55s\n",
      "814:\tlearn: 0.0001052\ttotal: 19.9s\tremaining: 4.53s\n",
      "815:\tlearn: 0.0001051\ttotal: 20s\tremaining: 4.5s\n",
      "816:\tlearn: 0.0001047\ttotal: 20s\tremaining: 4.48s\n",
      "817:\tlearn: 0.0001046\ttotal: 20s\tremaining: 4.46s\n",
      "818:\tlearn: 0.0001045\ttotal: 20.1s\tremaining: 4.43s\n",
      "819:\tlearn: 0.0001044\ttotal: 20.1s\tremaining: 4.41s\n",
      "820:\tlearn: 0.0001043\ttotal: 20.1s\tremaining: 4.38s\n",
      "821:\tlearn: 0.0001041\ttotal: 20.1s\tremaining: 4.36s\n",
      "822:\tlearn: 0.0001036\ttotal: 20.2s\tremaining: 4.33s\n",
      "823:\tlearn: 0.0001034\ttotal: 20.2s\tremaining: 4.31s\n",
      "824:\tlearn: 0.0001032\ttotal: 20.2s\tremaining: 4.28s\n",
      "825:\tlearn: 0.0001031\ttotal: 20.2s\tremaining: 4.26s\n",
      "826:\tlearn: 0.0001024\ttotal: 20.2s\tremaining: 4.23s\n",
      "827:\tlearn: 0.0001023\ttotal: 20.3s\tremaining: 4.21s\n",
      "828:\tlearn: 0.0001023\ttotal: 20.3s\tremaining: 4.18s\n",
      "829:\tlearn: 0.0001023\ttotal: 20.3s\tremaining: 4.16s\n",
      "830:\tlearn: 0.0001020\ttotal: 20.3s\tremaining: 4.13s\n",
      "831:\tlearn: 0.0001015\ttotal: 20.4s\tremaining: 4.11s\n",
      "832:\tlearn: 0.0001012\ttotal: 20.4s\tremaining: 4.08s\n",
      "833:\tlearn: 0.0001011\ttotal: 20.4s\tremaining: 4.06s\n",
      "834:\tlearn: 0.0001011\ttotal: 20.4s\tremaining: 4.03s\n",
      "835:\tlearn: 0.0001009\ttotal: 20.4s\tremaining: 4.01s\n",
      "836:\tlearn: 0.0001006\ttotal: 20.5s\tremaining: 3.98s\n",
      "837:\tlearn: 0.0001005\ttotal: 20.5s\tremaining: 3.96s\n",
      "838:\tlearn: 0.0001002\ttotal: 20.5s\tremaining: 3.94s\n",
      "839:\tlearn: 0.0000994\ttotal: 20.5s\tremaining: 3.91s\n",
      "840:\tlearn: 0.0000993\ttotal: 20.6s\tremaining: 3.89s\n",
      "841:\tlearn: 0.0000993\ttotal: 20.6s\tremaining: 3.86s\n",
      "842:\tlearn: 0.0000992\ttotal: 20.6s\tremaining: 3.84s\n",
      "843:\tlearn: 0.0000991\ttotal: 20.6s\tremaining: 3.81s\n",
      "844:\tlearn: 0.0000987\ttotal: 20.7s\tremaining: 3.79s\n",
      "845:\tlearn: 0.0000986\ttotal: 20.7s\tremaining: 3.77s\n",
      "846:\tlearn: 0.0000985\ttotal: 20.7s\tremaining: 3.74s\n",
      "847:\tlearn: 0.0000984\ttotal: 20.7s\tremaining: 3.72s\n",
      "848:\tlearn: 0.0000983\ttotal: 20.8s\tremaining: 3.69s\n",
      "849:\tlearn: 0.0000980\ttotal: 20.8s\tremaining: 3.67s\n",
      "850:\tlearn: 0.0000978\ttotal: 20.8s\tremaining: 3.65s\n",
      "851:\tlearn: 0.0000978\ttotal: 20.8s\tremaining: 3.62s\n",
      "852:\tlearn: 0.0000977\ttotal: 20.9s\tremaining: 3.6s\n",
      "853:\tlearn: 0.0000975\ttotal: 20.9s\tremaining: 3.57s\n",
      "854:\tlearn: 0.0000973\ttotal: 20.9s\tremaining: 3.55s\n",
      "855:\tlearn: 0.0000971\ttotal: 20.9s\tremaining: 3.52s\n",
      "856:\tlearn: 0.0000966\ttotal: 21s\tremaining: 3.5s\n",
      "857:\tlearn: 0.0000966\ttotal: 21s\tremaining: 3.47s\n",
      "858:\tlearn: 0.0000962\ttotal: 21s\tremaining: 3.45s\n",
      "859:\tlearn: 0.0000960\ttotal: 21s\tremaining: 3.43s\n",
      "860:\tlearn: 0.0000958\ttotal: 21.1s\tremaining: 3.4s\n",
      "861:\tlearn: 0.0000957\ttotal: 21.1s\tremaining: 3.38s\n",
      "862:\tlearn: 0.0000954\ttotal: 21.1s\tremaining: 3.35s\n",
      "863:\tlearn: 0.0000951\ttotal: 21.2s\tremaining: 3.33s\n",
      "864:\tlearn: 0.0000951\ttotal: 21.2s\tremaining: 3.31s\n",
      "865:\tlearn: 0.0000950\ttotal: 21.2s\tremaining: 3.28s\n",
      "866:\tlearn: 0.0000950\ttotal: 21.2s\tremaining: 3.26s\n",
      "867:\tlearn: 0.0000946\ttotal: 21.3s\tremaining: 3.23s\n",
      "868:\tlearn: 0.0000946\ttotal: 21.3s\tremaining: 3.21s\n",
      "869:\tlearn: 0.0000944\ttotal: 21.3s\tremaining: 3.18s\n",
      "870:\tlearn: 0.0000944\ttotal: 21.3s\tremaining: 3.16s\n",
      "871:\tlearn: 0.0000939\ttotal: 21.4s\tremaining: 3.14s\n",
      "872:\tlearn: 0.0000938\ttotal: 21.4s\tremaining: 3.11s\n",
      "873:\tlearn: 0.0000936\ttotal: 21.4s\tremaining: 3.09s\n",
      "874:\tlearn: 0.0000935\ttotal: 21.5s\tremaining: 3.06s\n",
      "875:\tlearn: 0.0000934\ttotal: 21.5s\tremaining: 3.04s\n",
      "876:\tlearn: 0.0000933\ttotal: 21.5s\tremaining: 3.02s\n",
      "877:\tlearn: 0.0000931\ttotal: 21.5s\tremaining: 2.99s\n",
      "878:\tlearn: 0.0000928\ttotal: 21.6s\tremaining: 2.97s\n",
      "879:\tlearn: 0.0000927\ttotal: 21.6s\tremaining: 2.94s\n",
      "880:\tlearn: 0.0000927\ttotal: 21.6s\tremaining: 2.92s\n",
      "881:\tlearn: 0.0000920\ttotal: 21.6s\tremaining: 2.89s\n",
      "882:\tlearn: 0.0000920\ttotal: 21.6s\tremaining: 2.87s\n",
      "883:\tlearn: 0.0000918\ttotal: 21.7s\tremaining: 2.84s\n",
      "884:\tlearn: 0.0000916\ttotal: 21.7s\tremaining: 2.82s\n",
      "885:\tlearn: 0.0000910\ttotal: 21.7s\tremaining: 2.79s\n",
      "886:\tlearn: 0.0000910\ttotal: 21.7s\tremaining: 2.77s\n",
      "887:\tlearn: 0.0000908\ttotal: 21.8s\tremaining: 2.75s\n",
      "888:\tlearn: 0.0000906\ttotal: 21.8s\tremaining: 2.72s\n",
      "889:\tlearn: 0.0000903\ttotal: 21.8s\tremaining: 2.69s\n",
      "890:\tlearn: 0.0000903\ttotal: 21.8s\tremaining: 2.67s\n",
      "891:\tlearn: 0.0000903\ttotal: 21.9s\tremaining: 2.65s\n",
      "892:\tlearn: 0.0000902\ttotal: 21.9s\tremaining: 2.62s\n",
      "893:\tlearn: 0.0000900\ttotal: 21.9s\tremaining: 2.6s\n",
      "894:\tlearn: 0.0000898\ttotal: 21.9s\tremaining: 2.57s\n",
      "895:\tlearn: 0.0000898\ttotal: 22s\tremaining: 2.55s\n",
      "896:\tlearn: 0.0000897\ttotal: 22s\tremaining: 2.52s\n",
      "897:\tlearn: 0.0000896\ttotal: 22s\tremaining: 2.5s\n",
      "898:\tlearn: 0.0000896\ttotal: 22s\tremaining: 2.48s\n",
      "899:\tlearn: 0.0000896\ttotal: 22.1s\tremaining: 2.45s\n",
      "900:\tlearn: 0.0000895\ttotal: 22.1s\tremaining: 2.43s\n",
      "901:\tlearn: 0.0000895\ttotal: 22.1s\tremaining: 2.4s\n",
      "902:\tlearn: 0.0000894\ttotal: 22.1s\tremaining: 2.38s\n",
      "903:\tlearn: 0.0000893\ttotal: 22.2s\tremaining: 2.35s\n",
      "904:\tlearn: 0.0000890\ttotal: 22.2s\tremaining: 2.33s\n",
      "905:\tlearn: 0.0000888\ttotal: 22.2s\tremaining: 2.3s\n",
      "906:\tlearn: 0.0000886\ttotal: 22.2s\tremaining: 2.28s\n",
      "907:\tlearn: 0.0000886\ttotal: 22.2s\tremaining: 2.25s\n",
      "908:\tlearn: 0.0000885\ttotal: 22.3s\tremaining: 2.23s\n",
      "909:\tlearn: 0.0000881\ttotal: 22.3s\tremaining: 2.21s\n",
      "910:\tlearn: 0.0000881\ttotal: 22.3s\tremaining: 2.18s\n",
      "911:\tlearn: 0.0000880\ttotal: 22.4s\tremaining: 2.16s\n",
      "912:\tlearn: 0.0000879\ttotal: 22.4s\tremaining: 2.13s\n",
      "913:\tlearn: 0.0000876\ttotal: 22.4s\tremaining: 2.11s\n",
      "914:\tlearn: 0.0000876\ttotal: 22.4s\tremaining: 2.08s\n",
      "915:\tlearn: 0.0000876\ttotal: 22.5s\tremaining: 2.06s\n",
      "916:\tlearn: 0.0000875\ttotal: 22.5s\tremaining: 2.03s\n",
      "917:\tlearn: 0.0000874\ttotal: 22.5s\tremaining: 2.01s\n",
      "918:\tlearn: 0.0000873\ttotal: 22.5s\tremaining: 1.99s\n",
      "919:\tlearn: 0.0000871\ttotal: 22.6s\tremaining: 1.96s\n",
      "920:\tlearn: 0.0000870\ttotal: 22.6s\tremaining: 1.94s\n",
      "921:\tlearn: 0.0000864\ttotal: 22.6s\tremaining: 1.91s\n",
      "922:\tlearn: 0.0000863\ttotal: 22.6s\tremaining: 1.89s\n",
      "923:\tlearn: 0.0000862\ttotal: 22.6s\tremaining: 1.86s\n",
      "924:\tlearn: 0.0000860\ttotal: 22.7s\tremaining: 1.84s\n",
      "925:\tlearn: 0.0000859\ttotal: 22.7s\tremaining: 1.81s\n",
      "926:\tlearn: 0.0000857\ttotal: 22.7s\tremaining: 1.79s\n",
      "927:\tlearn: 0.0000854\ttotal: 22.7s\tremaining: 1.76s\n",
      "928:\tlearn: 0.0000853\ttotal: 22.8s\tremaining: 1.74s\n",
      "929:\tlearn: 0.0000852\ttotal: 22.8s\tremaining: 1.72s\n",
      "930:\tlearn: 0.0000847\ttotal: 22.8s\tremaining: 1.69s\n",
      "931:\tlearn: 0.0000844\ttotal: 22.8s\tremaining: 1.67s\n",
      "932:\tlearn: 0.0000843\ttotal: 22.9s\tremaining: 1.64s\n",
      "933:\tlearn: 0.0000842\ttotal: 22.9s\tremaining: 1.62s\n",
      "934:\tlearn: 0.0000841\ttotal: 22.9s\tremaining: 1.59s\n",
      "935:\tlearn: 0.0000840\ttotal: 22.9s\tremaining: 1.57s\n",
      "936:\tlearn: 0.0000838\ttotal: 23s\tremaining: 1.54s\n",
      "937:\tlearn: 0.0000838\ttotal: 23s\tremaining: 1.52s\n",
      "938:\tlearn: 0.0000837\ttotal: 23s\tremaining: 1.49s\n",
      "939:\tlearn: 0.0000836\ttotal: 23s\tremaining: 1.47s\n",
      "940:\tlearn: 0.0000835\ttotal: 23s\tremaining: 1.44s\n",
      "941:\tlearn: 0.0000834\ttotal: 23.1s\tremaining: 1.42s\n",
      "942:\tlearn: 0.0000834\ttotal: 23.1s\tremaining: 1.4s\n",
      "943:\tlearn: 0.0000833\ttotal: 23.1s\tremaining: 1.37s\n",
      "944:\tlearn: 0.0000832\ttotal: 23.1s\tremaining: 1.35s\n",
      "945:\tlearn: 0.0000831\ttotal: 23.2s\tremaining: 1.32s\n",
      "946:\tlearn: 0.0000829\ttotal: 23.2s\tremaining: 1.3s\n",
      "947:\tlearn: 0.0000827\ttotal: 23.2s\tremaining: 1.27s\n",
      "948:\tlearn: 0.0000826\ttotal: 23.2s\tremaining: 1.25s\n",
      "949:\tlearn: 0.0000824\ttotal: 23.3s\tremaining: 1.22s\n",
      "950:\tlearn: 0.0000824\ttotal: 23.3s\tremaining: 1.2s\n",
      "951:\tlearn: 0.0000823\ttotal: 23.3s\tremaining: 1.18s\n",
      "952:\tlearn: 0.0000821\ttotal: 23.3s\tremaining: 1.15s\n",
      "953:\tlearn: 0.0000817\ttotal: 23.4s\tremaining: 1.13s\n",
      "954:\tlearn: 0.0000816\ttotal: 23.4s\tremaining: 1.1s\n",
      "955:\tlearn: 0.0000816\ttotal: 23.4s\tremaining: 1.08s\n",
      "956:\tlearn: 0.0000814\ttotal: 23.4s\tremaining: 1.05s\n",
      "957:\tlearn: 0.0000813\ttotal: 23.5s\tremaining: 1.03s\n",
      "958:\tlearn: 0.0000812\ttotal: 23.5s\tremaining: 1s\n",
      "959:\tlearn: 0.0000808\ttotal: 23.5s\tremaining: 980ms\n",
      "960:\tlearn: 0.0000807\ttotal: 23.5s\tremaining: 955ms\n",
      "961:\tlearn: 0.0000806\ttotal: 23.6s\tremaining: 931ms\n",
      "962:\tlearn: 0.0000801\ttotal: 23.6s\tremaining: 906ms\n",
      "963:\tlearn: 0.0000801\ttotal: 23.6s\tremaining: 882ms\n",
      "964:\tlearn: 0.0000800\ttotal: 23.6s\tremaining: 857ms\n",
      "965:\tlearn: 0.0000800\ttotal: 23.7s\tremaining: 833ms\n",
      "966:\tlearn: 0.0000795\ttotal: 23.7s\tremaining: 808ms\n",
      "967:\tlearn: 0.0000795\ttotal: 23.7s\tremaining: 784ms\n",
      "968:\tlearn: 0.0000794\ttotal: 23.7s\tremaining: 759ms\n",
      "969:\tlearn: 0.0000793\ttotal: 23.8s\tremaining: 735ms\n",
      "970:\tlearn: 0.0000793\ttotal: 23.8s\tremaining: 710ms\n",
      "971:\tlearn: 0.0000791\ttotal: 23.8s\tremaining: 686ms\n",
      "972:\tlearn: 0.0000790\ttotal: 23.8s\tremaining: 661ms\n",
      "973:\tlearn: 0.0000790\ttotal: 23.8s\tremaining: 637ms\n",
      "974:\tlearn: 0.0000789\ttotal: 23.9s\tremaining: 612ms\n",
      "975:\tlearn: 0.0000789\ttotal: 23.9s\tremaining: 587ms\n",
      "976:\tlearn: 0.0000788\ttotal: 23.9s\tremaining: 563ms\n",
      "977:\tlearn: 0.0000787\ttotal: 23.9s\tremaining: 538ms\n",
      "978:\tlearn: 0.0000784\ttotal: 24s\tremaining: 514ms\n",
      "979:\tlearn: 0.0000781\ttotal: 24s\tremaining: 489ms\n",
      "980:\tlearn: 0.0000781\ttotal: 24s\tremaining: 465ms\n",
      "981:\tlearn: 0.0000779\ttotal: 24s\tremaining: 440ms\n",
      "982:\tlearn: 0.0000777\ttotal: 24.1s\tremaining: 416ms\n",
      "983:\tlearn: 0.0000776\ttotal: 24.1s\tremaining: 391ms\n",
      "984:\tlearn: 0.0000775\ttotal: 24.1s\tremaining: 367ms\n",
      "985:\tlearn: 0.0000774\ttotal: 24.1s\tremaining: 343ms\n",
      "986:\tlearn: 0.0000773\ttotal: 24.1s\tremaining: 318ms\n",
      "987:\tlearn: 0.0000771\ttotal: 24.2s\tremaining: 294ms\n",
      "988:\tlearn: 0.0000768\ttotal: 24.2s\tremaining: 269ms\n",
      "989:\tlearn: 0.0000767\ttotal: 24.2s\tremaining: 245ms\n",
      "990:\tlearn: 0.0000766\ttotal: 24.2s\tremaining: 220ms\n",
      "991:\tlearn: 0.0000766\ttotal: 24.3s\tremaining: 196ms\n",
      "992:\tlearn: 0.0000761\ttotal: 24.3s\tremaining: 171ms\n",
      "993:\tlearn: 0.0000760\ttotal: 24.3s\tremaining: 147ms\n",
      "994:\tlearn: 0.0000757\ttotal: 24.3s\tremaining: 122ms\n",
      "995:\tlearn: 0.0000757\ttotal: 24.4s\tremaining: 97.8ms\n",
      "996:\tlearn: 0.0000755\ttotal: 24.4s\tremaining: 73.4ms\n",
      "997:\tlearn: 0.0000754\ttotal: 24.4s\tremaining: 48.9ms\n",
      "998:\tlearn: 0.0000752\ttotal: 24.4s\tremaining: 24.5ms\n",
      "999:\tlearn: 0.0000752\ttotal: 24.5s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1a70e3f3c40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cbc=CatBoostClassifier()\n",
    "cbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for train: 100.00\n",
      "\n",
      "Classification Report:                  0      1  accuracy  macro avg  weighted avg\n",
      "precision       1.0    1.0       1.0        1.0           1.0\n",
      "recall          1.0    1.0       1.0        1.0           1.0\n",
      "f1-score        1.0    1.0       1.0        1.0           1.0\n",
      "support    159221.0  270.0       1.0   159491.0      159491.0\n",
      "\n",
      "Confusion Mat: \n",
      " [[159221      0]\n",
      " [     0    270]]\n",
      "Result for test data\n",
      "Accuracy for test: 99.95\n",
      "\n",
      "Classification Report:                      0           1  accuracy     macro avg  weighted avg\n",
      "precision      0.999566    0.954887  0.999497      0.977227      0.999481\n",
      "recall         0.999930    0.774390  0.999497      0.887160      0.999497\n",
      "f1-score       0.999748    0.855219  0.999497      0.927483      0.999471\n",
      "support    85279.000000  164.000000  0.999497  85443.000000  85443.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[85273     6]\n",
      " [   37   127]]\n"
     ]
    }
   ],
   "source": [
    "y_trainpred=cbc.predict(X_train)\n",
    "y_testpred=cbc.predict(X_test)\n",
    "acc_score(y_train,y_trainpred,train=True)\n",
    "acc_score(y_test,y_testpred,train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictscore['Catb']={\n",
    "    'Train':f1_score(y_train,y_trainpred),\n",
    "    'Test':f1_score(y_test,y_testpred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 270, number of negative: 159221\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 159491, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001693 -> initscore=-6.379626\n",
      "[LightGBM] [Info] Start training from score -6.379626\n",
      "Accuracy for train: 99.75\n",
      "\n",
      "Classification Report:                       0           1  accuracy      macro avg   weighted avg\n",
      "precision       0.999377    0.360759  0.997479       0.680068       0.998296\n",
      "recall          0.998097    0.633333  0.997479       0.815715       0.997479\n",
      "f1-score        0.998737    0.459677  0.997479       0.729207       0.997824\n",
      "support    159221.000000  270.000000  0.997479  159491.000000  159491.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[158918    303]\n",
      " [    99    171]]\n",
      "Result for test data\n",
      "Accuracy for test: 99.56\n",
      "\n",
      "Classification Report:                      0           1  accuracy     macro avg  weighted avg\n",
      "precision      0.998989    0.210811  0.995576      0.604900      0.997476\n",
      "recall         0.996576    0.475610  0.995576      0.736093      0.995576\n",
      "f1-score       0.997781    0.292135  0.995576      0.644958      0.996427\n",
      "support    85279.000000  164.000000  0.995576  85443.000000  85443.000000\n",
      "\n",
      "Confusion Mat: \n",
      " [[84987   292]\n",
      " [   86    78]]\n"
     ]
    }
   ],
   "source": [
    "lgbm=LGBMClassifier()\n",
    "lgbm.fit(X_train,y_train)\n",
    "y_trainpred=lgbm.predict(X_train)\n",
    "y_testpred=lgbm.predict(X_test)\n",
    "\n",
    "acc_score(y_train,y_trainpred,train=True)\n",
    "acc_score(y_test,y_testpred,train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictscore['LGBM']={\n",
    "    'Train':f1_score(y_train,y_trainpred),\n",
    "    'Test':f1_score(y_test,y_testpred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have prepared all the different kinds of models and will be comparing them\n",
    "# The model comparison will be done on the basis of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANNs</th>\n",
       "      <th>Randomforest</th>\n",
       "      <th>XGB</th>\n",
       "      <th>Catb</th>\n",
       "      <th>LGBM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.851324</td>\n",
       "      <td>0.998145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.789298</td>\n",
       "      <td>0.841060</td>\n",
       "      <td>0.852349</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.292135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ANNs  Randomforest       XGB      Catb      LGBM\n",
       "Train  0.851324      0.998145  1.000000  1.000000  0.459677\n",
       "Test   0.789298      0.841060  0.852349  0.855219  0.292135"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparedf=pd.DataFrame(dictscore)\n",
    "comparedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparedf.plot(kind='barh',figsize=(13,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing this we can see that CatBoost prooves to be a better model choice as compared to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
